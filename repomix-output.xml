This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
app.py
CONTRIBUTING.md
docker-compose.yml
Dockerfile
PROCESSING_STRUCTURE.md
README.md
requirements.txt
SETUP.md
static/script.js
static/style.css
templates/index.html
utils/clustering.py
utils/downloader.py
utils/keyframes.py
utils/summarizer.py
utils/topic_analyzer.py
utils/transcriber.py
utils/translator.py
utils/tts.py
utils/video_maker.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="utils/topic_analyzer.py">
"""
Topic Analyzer Module

This module provides advanced topic analysis for video transcripts:
1. Dynamic topic extraction based on content
2. Topic naming based on keywords and context
3. Timestamp-based segmentation for better keyframe selection
"""

import logging
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import string
import re
from collections import defaultdict
import spacy
from gensim import corpora
from gensim.models import LdaModel
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOPWORDS
import yake

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    logger.info("Downloading NLTK data...")
    nltk.download('punkt')
    nltk.download('stopwords')

# Load spaCy model for NER and topic extraction
try:
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
except (ImportError, OSError):
    logger.warning("spaCy model not available. Some features will be limited.")
    SPACY_AVAILABLE = False

# Language-specific settings
LANGUAGE_STOPWORDS = {
    'en': set(stopwords.words('english')),
    'hi': set(stopwords.words('hindi') if 'hindi' in stopwords.fileids() else []),
    'kn': set(stopwords.words('kannada') if 'kannada' in stopwords.fileids() else [])
}

# Add custom stopwords for each language
CUSTOM_STOPWORDS = {
    'en': {'like', 'one', 'would', 'get', 'also', 'could', 'may', 'even', 'much', 'many', 'well', 
           'thing', 'things', 'way', 'something', 'everything', 'anything', 'nothing', 'say', 'said',
           'know', 'going', 'go', 'goes', 'went', 'gone', 'just', 'good', 'right', 'now', 'see', 'come'},
    'hi': set(),
    'kn': set()
}

# Combine NLTK and custom stopwords
for lang in LANGUAGE_STOPWORDS:
    LANGUAGE_STOPWORDS[lang].update(CUSTOM_STOPWORDS.get(lang, set()))


def preprocess_text(text: str, language: str = 'en') -> str:
    """Clean and preprocess text for analysis."""
    if not isinstance(text, str):
        return ""
        
    # Convert to lowercase
    text = text.lower()
    
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Remove punctuation (language-specific handling)
    if language == 'en':
        # Keep sentence boundaries for English
        text = re.sub(r'[^\w\s.!?]', '', text)
    else:
        # For other languages, be more conservative with punctuation removal
        text = re.sub(r'[^\w\s]', ' ', text)
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    return text


def tokenize_text(text: str, language: str = 'en') -> List[str]:
    """Tokenize text into words with stopword removal."""
    if not text.strip():
        return []
        
    # Tokenize
    tokens = nltk.word_tokenize(text, language='english' if language == 'en' else language)
    
    # Remove stopwords and short tokens
    tokens = [
        token for token in tokens 
        if token not in LANGUAGE_STOPWORDS.get(language, set()) 
        and len(token) > 2
        and not token.isdigit()
    ]
    
    return tokens


def determine_optimal_clusters(texts: List[str], max_clusters: int = 10, min_clusters: int = 2) -> int:
    """
    Determine the optimal number of clusters using silhouette score.
    
    Args:
        texts: List of text segments
        max_clusters: Maximum number of clusters to consider
        min_clusters: Minimum number of clusters to consider
        
    Returns:
        Optimal number of clusters
    """
    if len(texts) < min_clusters:
        return min(len(texts), 1)
    
    # Vectorize texts
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(texts)
    
    # Try different numbers of clusters
    best_score = -1
    best_n_clusters = min_clusters
    
    max_clusters = min(max_clusters, len(texts) - 1)
    
    for n_clusters in range(min_clusters, max_clusters + 1):
        # Skip if we have too few samples for the number of clusters
        if n_clusters >= len(texts):
            continue
            
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X)
            
            # Calculate silhouette score
            score = silhouette_score(X, labels)
            
            if score > best_score:
                best_score = score
                best_n_clusters = n_clusters
        except Exception as e:
            logger.warning(f"Error calculating silhouette score for {n_clusters} clusters: {e}")
    
    logger.info(f"Optimal number of clusters determined: {best_n_clusters} (score: {best_score:.3f})")
    return best_n_clusters


def extract_named_entities(texts: List[str]) -> List[str]:
    """
    Extract named entities from texts using spaCy.
    
    Args:
        texts: List of text segments
        
    Returns:
        List of named entities
    """
    if not SPACY_AVAILABLE:
        return []
    
    entities = []
    combined_text = " ".join(texts[:20])  # Limit to first 20 segments for performance
    
    try:
        doc = nlp(combined_text)
        for ent in doc.ents:
            if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LOC', 'PRODUCT', 'EVENT']:
                entities.append(ent.text)
    except Exception as e:
        logger.error(f"Error extracting named entities: {e}")
    
    # Count frequencies and return top entities
    entity_counts = defaultdict(int)
    for entity in entities:
        entity_counts[entity] += 1
    
    # Return top 10 entities
    return [entity for entity, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]]


def extract_topic_keywords(texts: List[str], num_keywords: int = 5, language: str = 'en') -> List[str]:
    """
    Extract keywords that best represent a topic using YAKE.
    
    Args:
        texts: List of text segments
        num_keywords: Number of keywords to extract
        language: Language code
        
    Returns:
        List of keywords
    """
    try:
        # Combine texts
        combined_text = " ".join(texts)
        
        # Use YAKE for keyword extraction
        language_code = 'en' if language not in ['en', 'hi', 'kn'] else language
        kw_extractor = yake.KeywordExtractor(
            lan=language_code, 
            n=2,  # Extract 1-2 word keywords
            dedupLim=0.7,  # Avoid similar keywords
            dedupFunc='seqm',
            windowsSize=2,
            top=num_keywords
        )
        
        keywords = kw_extractor.extract_keywords(combined_text)
        
        # Return just the keywords (not scores)
        return [kw[0] for kw in keywords]
    except Exception as e:
        logger.error(f"Error extracting keywords with YAKE: {e}")
        
        # Fallback to simple word frequency
        word_freq = defaultdict(int)
        for text in texts:
            for word in tokenize_text(text, language):
                word_freq[word] += 1
        
        # Return top words
        return [word for word, _ in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:num_keywords]]


def generate_topic_name(keywords: List[str], entities: List[str] = None) -> str:
    """
    Generate a descriptive name for a topic based on keywords and entities.
    
    Args:
        keywords: List of keywords for the topic
        entities: List of named entities (optional)
        
    Returns:
        Topic name as a string
    """
    if not keywords:
        return "Miscellaneous"
    
    # Combine keywords and entities
    if entities:
        # Try to find a good entity to use
        for entity in entities:
            # Check if any keyword is part of this entity
            for keyword in keywords[:3]:
                if keyword.lower() in entity.lower():
                    # Found a good match
                    return entity
    
    # Default: use the first 2-3 keywords
    if len(keywords) >= 3:
        return f"{keywords[0].title()} & {keywords[1].title()}"
    elif len(keywords) == 2:
        return f"{keywords[0].title()} & {keywords[1].title()}"
    else:
        return keywords[0].title()


def analyze_topic_segments(segments: List[Dict[str, Any]], language: str = 'en') -> List[Dict[str, Any]]:
    """
    Analyze segments to identify distinct topics with meaningful names.
    
    Args:
        segments: List of transcript segments with 'text' and timestamp fields
        language: Language code
        
    Returns:
        List of topic dictionaries with name, keywords, start/end times
    """
    # Extract text from segments
    texts = [seg.get('text', '') for seg in segments]
    preprocessed_texts = [preprocess_text(text, language) for text in texts]
    
    # Determine optimal number of topics
    n_topics = determine_optimal_clusters(
        preprocessed_texts, 
        max_clusters=min(8, max(2, len(segments) // 10)),  # Adaptive range
        min_clusters=2
    )
    
    # Vectorize texts
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(preprocessed_texts)
    
    # Cluster segments
    kmeans = KMeans(n_clusters=n_topics, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X)
    
    # Assign cluster labels to segments
    for i, seg in enumerate(segments):
        seg['topic_id'] = int(cluster_labels[i])
    
    # Group segments by topic
    topics = defaultdict(list)
    for seg in segments:
        topics[seg['topic_id']].append(seg)
    
    # Process each topic
    topic_results = []
    for topic_id, topic_segments in topics.items():
        # Sort segments by timestamp
        topic_segments.sort(key=lambda x: x.get('start', 0))
        
        # Extract topic text
        topic_texts = [seg.get('text', '') for seg in topic_segments]
        
        # Extract named entities
        entities = extract_named_entities(topic_texts)
        
        # Extract keywords
        keywords = extract_topic_keywords(topic_texts, num_keywords=5, language=language)
        
        # Generate topic name
        topic_name = generate_topic_name(keywords, entities)
        
        # Get start and end times
        start_time = min(seg.get('start', 0) for seg in topic_segments)
        end_time = max(seg.get('end', 0) for seg in topic_segments)
        
        # Create topic result
        topic_result = {
            'topic_id': topic_id,
            'name': topic_name,
            'keywords': keywords,
            'entities': entities,
            'start_time': start_time,
            'end_time': end_time,
            'segments': topic_segments,
            'segment_count': len(topic_segments)
        }
        
        topic_results.append(topic_result)
    
    # Sort topics by start time
    topic_results.sort(key=lambda x: x['start_time'])
    
    # Re-assign topic IDs in chronological order
    for i, topic in enumerate(topic_results):
        topic['topic_id'] = i
        for seg in topic['segments']:
            seg['topic_id'] = i
    
    logger.info(f"Identified {len(topic_results)} topics with names: {[t['name'] for t in topic_results]}")
    return topic_results


def get_keyframes_for_topic(keyframes: List[Dict[str, Any]], start_time: float, end_time: float) -> List[Dict[str, Any]]:
    """
    Select keyframes that fall within a topic's time range.
    
    Args:
        keyframes: List of keyframe metadata dictionaries
        start_time: Topic start time in seconds
        end_time: Topic end time in seconds
        
    Returns:
        List of keyframes within the topic time range
    """
    # Filter keyframes by timestamp
    topic_keyframes = []
    
    for kf in keyframes:
        timestamp = kf.get('timestamp')
        if timestamp is not None and start_time <= timestamp <= end_time:
            topic_keyframes.append(kf)
    
    return topic_keyframes


def calculate_keyframe_distribution(topic_duration: float, min_keyframes: int = 10, max_keyframes: int = 40) -> int:
    """
    Calculate how many keyframes to use based on topic duration.
    
    Args:
        topic_duration: Duration of topic in seconds
        min_keyframes: Minimum number of keyframes
        max_keyframes: Maximum number of keyframes
        
    Returns:
        Target number of keyframes
    """
    # Base calculation: 1 keyframe per 2 seconds, with limits
    target_keyframes = int(topic_duration / 2)
    
    # Apply limits
    target_keyframes = max(min_keyframes, min(max_keyframes, target_keyframes))
    
    return target_keyframes
</file>

<file path="CONTRIBUTING.md">
# Contributing to Video Summarization Web App

Thank you for your interest in contributing! Here's how you can help improve this project.

## ğŸš€ Getting Started

1. **Fork** the repository on GitHub
2. **Clone** your forked repository
   ```bash
   git clone https://github.com/your-username/video-summarization-app.git
   cd video-summarization-app
   ```
3. **Set up** the development environment:
   ```bash
   # Create and activate virtual environment (Windows)
   python -m venv venv
   .\venv\Scripts\activate
   
   # Install dependencies
   pip install -r requirements-dev.txt
   
   # Install pre-commit hooks
   pre-commit install
   ```
4. **Run tests** to verify your setup:
   ```bash
   python -m pytest
   ```

## ğŸ›  Development Workflow

1. Create a new branch for your feature or bugfix:
   ```bash
   git checkout -b feature/your-feature-name
   # or
   git checkout -b bugfix/issue-number-description
   ```

2. Make your changes following the code style guidelines

3. Write or update tests as needed

4. Run tests and fix any issues:
   ```bash
   python -m pytest
   ```

5. Commit your changes with a descriptive message:
   ```bash
   git add .
   git commit -m "feat: add new feature"
   # or
   git commit -m "fix: resolve issue with video processing"
   ```

6. Push your changes to your fork:
   ```bash
   git push origin your-branch-name
   ```

7. Open a **Pull Request** against the `main` branch

## ğŸ“ Code Style

- Follow [PEP 8](https://www.python.org/dev/peps/pep-0008/) for Python code
- Use type hints for function signatures
- Write docstrings for all public functions and classes
- Keep lines under 88 characters (Black's default line length)
- Use meaningful variable and function names

## ğŸ§ª Testing

- Write unit tests for new features and bug fixes
- Ensure all tests pass before submitting a PR
- Use descriptive test function names that describe the behavior being tested
- Follow the Arrange-Act-Assert pattern in tests

## ğŸ“š Documentation

- Update the README.md for significant changes
- Add docstrings to new functions and classes
- Document any new environment variables or configuration options

## ğŸ› Reporting Issues

When reporting issues, please include:

1. A clear, descriptive title
2. Steps to reproduce the issue
3. Expected vs. actual behavior
4. Environment details (OS, Python version, etc.)
5. Any relevant error messages or logs

## ğŸ¤ Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## ğŸ“œ License

By contributing, you agree that your contributions will be licensed under the project's [LICENSE](LICENSE) file.

## ğŸ™ Thank You!

Your contributions make open-source a fantastic place to learn, inspire, and create. Thank you for being part of our community!
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  app:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./uploads:/app/uploads
      - ./processed:/app/processed
    environment:
      - FLASK_APP=app.py
      - FLASK_ENV=production
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY:-dev-key-change-in-production}
      - MAX_CONTENT_LENGTH=41943040
      - MAX_VIDEO_DURATION=2400
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Optional: Add Redis for caching and background task queue in the future
  # redis:
  #   image: redis:alpine
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   restart: unless-stopped

# volumes:
#   redis_data:
</file>

<file path="Dockerfile">
# Use Python 3.9 slim as the base image
FROM python:3.9-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV FLASK_APP=app.py
ENV FLASK_ENV=production

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsm6 \
    libxext6 \
    && rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Download NLTK data
RUN python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"

# Copy the rest of the application code
COPY . .

# Create necessary directories
RUN mkdir -p uploads processed

# Expose the port the app runs on
EXPOSE 5000

# Command to run the application
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "app:app"]
</file>

<file path="PROCESSING_STRUCTURE.md">
# Video Processing Structure

## Overview
After processing a video, the system creates a well-organized folder structure in the `processed/` directory.

## Folder Structure
```
processed/
â””â”€â”€ {video_id}/
    â”œâ”€â”€ keyframes/           # All extracted keyframes
    â”œâ”€â”€ audio/              # TTS audio files for each topic
    â”œâ”€â”€ videos/             # Generated summary videos
    â”œâ”€â”€ metadata.json       # Complete video metadata
    â””â”€â”€ topic_{n}_metadata.json  # Individual topic metadata
```

## File Organization

### Keyframes Directory
- Contains all extracted keyframes from the video
- Each keyframe includes timestamp, OCR text, and confidence scores
- Organized by extraction order and topic relevance

### Audio Directory
- TTS-generated audio summaries for each topic
- Files named: `topic_0_summary.wav`, `topic_1_summary.wav`, etc.
- High-quality audio optimized for video integration

### Videos Directory
- Generated summary videos for each topic
- Files named: `topic_0_summary.mp4`, `topic_1_summary.mp4`, etc.
- Each video contains relevant keyframes synchronized with audio

### Metadata Files
- `metadata.json`: Complete video processing information
- `topic_{n}_metadata.json`: Individual topic details including:
  - Start/end times
  - Summary text
  - Keywords
  - Keyframe count
  - Associated files

## Benefits of New Structure

1. **Better Organization**: All related files are grouped by video ID
2. **Easier Management**: Clear separation of different file types
3. **Scalability**: Easy to add new processing outputs
4. **Debugging**: Better error tracking and file location
5. **User Experience**: Faster file access and download

## Processing Flow

1. **Video Upload/Download** â†’ Creates unique video ID
2. **Transcription** â†’ Generates text segments with timestamps
3. **Keyframe Extraction** â†’ Extracts visual content with OCR
4. **Topic Clustering** â†’ Groups content into meaningful topics
5. **Summary Generation** â†’ Creates text summaries for each topic
6. **TTS Generation** â†’ Converts summaries to audio
7. **Video Creation** â†’ Combines keyframes and audio into videos
8. **File Organization** â†’ Structures all outputs in organized folders

## Key Improvements

- **More Keyframes**: 30+ keyframes per topic for smooth video feel
- **Better Clustering**: Improved K-means clustering with similarity thresholds
- **Organized Storage**: Logical folder structure for easy access
- **Enhanced Metadata**: Comprehensive information about each processing step
- **Efficient Processing**: Optimized algorithms for faster completion
</file>

<file path="utils/transcriber.py">
import os
import json
import logging
import whisper
import torch
from typing import Dict, List, Tuple, Optional
from pydub import AudioSegment
import subprocess

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# All major Indian languages supported by Whisper
# Based on the 22 official languages of India as per the Constitution
SUPPORTED_LANGUAGES = {
    # Major Indian Languages
    'hindi': 'hi',           # à¤¹à¤¿à¤¨à¥à¤¦à¥€ - Most widely spoken
    'bengali': 'bn',         # à¦¬à¦¾à¦‚à¦²à¦¾ - 2nd most spoken
    'telugu': 'te',          # à°¤à±†à°²à±à°—à± - Andhra Pradesh, Telangana
    'marathi': 'mr',         # à¤®à¤°à¤¾à¤ à¥€ - Maharashtra
    'tamil': 'ta',           # à®¤à®®à®¿à®´à¯ - Tamil Nadu
    'gujarati': 'gu',        # àª—à«àªœàª°àª¾àª¤à«€ - Gujarat
    'urdu': 'ur',            # Ø§Ø±Ø¯Ùˆ - Widely understood
    'kannada': 'kn',         # à²•à²¨à³à²¨à²¡ - Karnataka
    'odia': 'or',            # à¬“à¬¡à¬¼à¬¿à¬† - Odisha
    'malayalam': 'ml',       # à´®à´²à´¯à´¾à´³à´‚ - Kerala
    'punjabi': 'pa',         # à¨ªà©°à¨œà¨¾à¨¬à©€ - Punjab
    'assamese': 'as',        # à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ - Assam
    'maithili': 'mai',       # à¤®à¥ˆà¤¥à¤¿à¤²à¥€ - Bihar, Nepal
    'santali': 'sat',        # á±¥á±Ÿá±±á±›á±Ÿá±²á±¤ - Jharkhand, West Bengal
    'nepali': 'ne',          # à¤¨à¥‡à¤ªà¤¾à¤²à¥€ - Sikkim, West Bengal
    'kashmiri': 'ks',        # à¤•à¥‰à¤¶à¥à¤° / Ú©Ù²Ø´ÙØ± - Kashmir
    'konkani': 'gom',        # à¤•à¥‹à¤‚à¤•à¤£à¥€ - Goa
    'sindhi': 'sd',          # Ø³Ù†Ø¯Ú¾ÛŒ / à¤¸à¤¿à¤¨à¥à¤§à¥€ - 
    'dogri': 'doi',          # à¤¡à¥‹à¤—à¤°à¥€ - Jammu & Kashmir
    'manipuri': 'mni',       # à¦®à§ˆà¦‡à¦¤à§ˆà¦‡à¦²à§‹à¦¨à§ - Manipur
    'bodo': 'brx',           # à¤¬à¤°'/à¤¬à¤¡à¤¼à¥‹ - Assam
    'sanskrit': 'sa',        # à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤ - Classical language
    
    # International languages for comparison/translation
    'english': 'en',         # English - Widely used
    'arabic': 'ar',          # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - For Islamic content
    'chinese': 'zh',         # ä¸­æ–‡ - For international content
    'spanish': 'es',         # EspaÃ±ol - International
    'french': 'fr',          # FranÃ§ais - International
    'german': 'de',          # Deutsch - International
    'japanese': 'ja',        # æ—¥æœ¬èª - International
    'korean': 'ko',          # í•œêµ­ì–´ - International
    'russian': 'ru',         # Ğ ÑƒÑÑĞºĞ¸Ğ¹ - International
    'portuguese': 'pt',      # PortuguÃªs - International
}

# For backward compatibility
LANGUAGE_MAP = SUPPORTED_LANGUAGES

# Model size mapping for different languages
# Larger models are more accurate but slower and require more memory
# Recommendations based on language complexity and available training data
MODEL_SIZES = {
    # Well-supported languages with large datasets
    'en': 'base',       # English - excellent support
    'hi': 'medium',     # Hindi - good support
    'bn': 'medium',     # Bengali - good support
    'ta': 'medium',     # Tamil - good support
    'te': 'medium',     # Telugu - good support
    'mr': 'medium',     # Marathi - good support
    'gu': 'medium',     # Gujarati - good support
    'ur': 'medium',     # Urdu - good support
    'kn': 'medium',     # Kannada - good support
    'ml': 'medium',     # Malayalam - good support
    'pa': 'medium',     # Punjabi - good support
    
    # Languages with moderate support - use larger model for better accuracy
    'or': 'large',      # Odia - moderate support
    'as': 'large',      # Assamese - moderate support
    'mai': 'large',     # Maithili - limited support
    'ne': 'large',      # Nepali - moderate support
    'sa': 'large',      # Sanskrit - specialized
    
    # Languages with limited support - use largest model available
    'sat': 'large',     # Santali - limited support
    'ks': 'large',      # Kashmiri - limited support
    'gom': 'large',     # Konkani - limited support
    'sd': 'large',      # Sindhi - limited support
    'doi': 'large',     # Dogri - very limited support
    'mni': 'large',     # Manipuri - limited support
    'brx': 'large',     # Bodo - limited support
    
    # International languages
    'ar': 'medium',     # Arabic - good support
    'zh': 'medium',     # Chinese - good support
    'es': 'base',       # Spanish - excellent support
    'fr': 'base',       # French - excellent support
    'de': 'base',       # German - excellent support
    'ja': 'medium',     # Japanese - good support
    'ko': 'medium',     # Korean - good support
    'ru': 'medium',     # Russian - good support
    'pt': 'base',       # Portuguese - good support
}

def extract_audio(
    video_path: str, 
    output_path: str, 
    sample_rate: int = 16000
) -> bool:
    """Extract audio from video using ffmpeg.
    
    Args:
        video_path: Path to input video file
        output_path: Path to save extracted audio
        sample_rate: Sample rate for output audio (Hz)
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        cmd = [
            'ffmpeg',
            '-y',  # Overwrite output file if it exists
            '-i', video_path,
            '-vn',  # Disable video
            '-acodec', 'pcm_s16le',  # 16-bit PCM
            '-ar', str(sample_rate),  # Sample rate
            '-ac', '1',  # Mono audio
            '-f', 'wav',
            output_path
        ]
        
        result = subprocess.run(
            cmd, 
            check=True, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            text=True
        )
        logger.debug(f"FFmpeg audio extraction output: {result.stdout}")
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Error extracting audio: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error in extract_audio: {str(e)}", exc_info=True)
        return False

def transcribe_video(
    video_path: str, 
    processed_dir: str, 
    video_id: str, 
    language: str = 'english',
    model_size: Optional[str] = None
) -> Tuple[Optional[str], List[Dict]]:
    """Transcribe video to text using Whisper.
    
    Args:
        video_path: Path to input video file
        processed_dir: Directory to save output files
        video_id: Unique ID for the video
        language: Language of the video ('english', 'hindi', or 'kannada')
        model_size: Override the default model size (tiny, base, small, medium, large)
        
    Returns:
        Tuple containing:
        - Path to segments JSON file (or None if failed)
        - List of segment dictionaries
    """
    try:
        # Validate language
        language = language.lower()
        if language not in LANGUAGE_MAP:
            logger.warning(f"Unsupported language: {language}. Defaulting to English.")
            language = 'english'
            
        lang_code = LANGUAGE_MAP[language]
        
        # Set model size if not provided
        if model_size is None:
            model_size = MODEL_SIZES.get(lang_code, 'base')
        
        logger.info(f"Transcribing video in {language} using Whisper {model_size} model...")
        
        # Create output directory if it doesn't exist
        os.makedirs(processed_dir, exist_ok=True)
        
        # Paths for intermediate and output files
        audio_path = os.path.join(processed_dir, f"{video_id}_audio.wav")
        seg_path = os.path.join(processed_dir, f"{video_id}_segments.json")
        
        # Extract audio from video
        if not extract_audio(video_path, audio_path):
            logger.error("Failed to extract audio from video")
            return None, []
        
        # Check if GPU is available
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"Using device: {device}")
        
        # Load Whisper model
        model = whisper.load_model(model_size, device=device)
        
        # Transcribe audio
        result = model.transcribe(
            audio_path,
            language=lang_code,
            verbose=True,
            fp16=(device == 'cuda')  # Use mixed precision on GPU
        )
        
        # Get segments
        segments = result.get('segments', [])
        
        # Save segments to file
        with open(seg_path, 'w', encoding='utf-8') as f:
            json.dump(segments, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Transcription complete. Saved to {seg_path}")
        return seg_path, segments
        
    except Exception as e:
        logger.error(f"Error in transcribe_video: {str(e)}", exc_info=True)
        return None, []
    finally:
        # Clean up temporary audio file if it exists
        if 'audio_path' in locals() and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
            except Exception as e:
                logger.warning(f"Failed to remove temporary audio file: {e}")
</file>

<file path="README.md">
# ğŸ¬ AI-Powered Video Summarizer

An intelligent video summarization system that automatically processes long videos and creates concise, meaningful summaries with AI-generated voiceovers and visual highlights. Built with comprehensive support for all major Indian languages and international languages.

## ğŸŒŸ Key Features

### ğŸ¯ Core Functionality
- **Multi-Source Input**: Supports YouTube URLs and direct video uploads
- **Intelligent Processing**: Extracts subtitles, audio, keyframes, and performs topic clustering
- **Dynamic Topic Analysis**: Automatically identifies the optimal number of topics based on content
- **Meaningful Topic Names**: Generates descriptive names for each topic instead of generic labels
- **Smart Keyframe Selection**: Selects relevant keyframes for each topic based on timestamps
- **AI Summarization**: Creates topic-based summaries with natural language processing
- **Video Generation**: Produces summarized videos with keyframes and AI voiceovers
- **Smooth Transitions**: Adds fade transitions between keyframes for better visual experience
- **Interactive Topics**: Users can select specific topics for detailed explanation

### ğŸŒ Language Support
- **22 Official Indian Languages**: Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Urdu, Kannada, Malayalam, Punjabi, Odia, Assamese, Nepali, Sanskrit, and more
- **International Languages**: English, Arabic, Chinese, Spanish, French, German, Japanese, Korean, Russian, Portuguese, and others
- **Language Translation**: Automatic translation between any supported languages
- **Native Voice Support**: High-quality Text-to-Speech in all supported languages

### ğŸ” Advanced Features
- **OCR Analysis**: Extracts text from video frames for enhanced context
- **Topic Clustering**: Uses advanced ML algorithms (LDA, NMF, K-means) for content organization
- **Keyframe Intelligence**: Smart keyframe extraction with similarity detection
- **Multiple Resolutions**: Support for 480p, 720p, and 1080p output
- **Progress Tracking**: Real-time processing status updates

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11
- FFmpeg installed on your system
- At least 4GB RAM (8GB recommended for large videos)
- Internet connection for translation services

### Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/meghanavemala/major-project
   cd video-summarizer
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install System Dependencies**

   **Windows:**
   - Install FFmpeg from https://ffmpeg.org/download.html
   - Add FFmpeg to system PATH
   - Install Tesseract OCR from https://github.com/UB-Mannheim/tesseract/wiki

   **macOS:**
   ```bash
   brew install ffmpeg tesseract
   ```

   **Ubuntu/Debian:**
   ```bash
   sudo apt update
   sudo apt install ffmpeg tesseract-ocr tesseract-ocr-hin tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-tel tesseract-ocr-kan tesseract-ocr-mal tesseract-ocr-urd tesseract-ocr-guj tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-mar tesseract-ocr-nep tesseract-ocr-san
   ```

5. **Download NLTK Data**
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('punkt_tab')"
   ```

6. **Create Required Directories**
   ```bash
   mkdir uploads processed static templates
   ```

### Running the Application

1. **Start the Flask Server**
   ```bash
   python app.py
   ```

2. **Access the Application**
   Open your browser and navigate to: `http://localhost:5000`

## ğŸ“– How It Works

### Step-by-Step Process

1. **Video Input**: User uploads a video file or provides a YouTube URL
2. **Language Selection**: User selects source language and target language for summary
3. **Processing Pipeline**:
   - **Audio Extraction**: Extracts audio from video using FFmpeg
   - **Transcription**: Uses OpenAI Whisper for speech-to-text conversion
   - **Keyframe Extraction**: Identifies important visual moments with timestamp data
   - **OCR Analysis**: Extracts text from keyframes for additional context
   - **Advanced Topic Analysis**: 
     - Analyzes transcript to determine optimal number of topics
     - Identifies distinct topics with natural boundaries
     - Generates meaningful names for each topic based on content
     - Maps topics to specific video time segments
   - **Smart Keyframe Selection**:
     - Selects keyframes relevant to each topic based on timestamps
     - Calculates optimal number of keyframes based on topic duration
     - Extracts additional keyframes if needed for smooth video generation
   - **Summarization**: Generates concise, informative summaries for each topic
   - **Translation**: Converts summaries to target language if needed
   - **Voice Generation**: Creates AI voiceovers using gTTS
   - **Enhanced Video Assembly**: 
     - Aligns keyframes with audio duration for perfect timing
     - Adds smooth fade transitions between keyframes
     - Creates topic-specific videos with descriptive names

4. **Output**: Interactive interface showing topic-based summaries with playable videos

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Flask API     â”‚    â”‚   Processing    â”‚
â”‚   (HTML/JS)     â”‚â—„â”€â”€â–ºâ”‚   (app.py)      â”‚â—„â”€â”€â–ºâ”‚   Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Utils Modules           â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ transcriber.py (Whisper)      â”‚
                    â”‚ â€¢ keyframes.py (CV + OCR)       â”‚
                    â”‚ â€¢ clustering.py (ML/NLP)        â”‚
                    â”‚ â€¢ topic_analyzer.py (Advanced)  â”‚
                    â”‚ â€¢ summarizer.py (Transformers)  â”‚
                    â”‚ â€¢ translator.py (Multi-lang)    â”‚
                    â”‚ â€¢ tts.py (Voice Generation)     â”‚
                    â”‚ â€¢ video_maker.py (Assembly)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Configuration

### Environment Variables
Create a `.env` file in the project root:

```env
# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-here
FLASK_ENV=production

# Processing Limits
MAX_CONTENT_LENGTH=104857600  # 100MB
MAX_VIDEO_DURATION=3600       # 60 minutes

# API Keys (Optional)
GOOGLE_TRANSLATE_API_KEY=your-google-api-key
AZURE_TRANSLATE_KEY=your-azure-key

# OCR Configuration
TESSERACT_CMD=/usr/bin/tesseract  # Path to tesseract binary
ENABLE_OCR=true

# Model Configuration
WHISPER_MODEL_SIZE=medium
SUMMARIZATION_MODEL=facebook/bart-large-cnn
TRANSLATION_METHOD=google  # google, m2m100, auto
```

### Processing Settings

You can customize processing parameters in each utility module:

- **Transcription**: Model size, language detection threshold
- **Keyframes**: Frame interval, similarity threshold, resolution
- **Clustering**: Number of topics, clustering method, minimum topic size
- **Summarization**: Summary length, model selection
- **Translation**: Translation service, fallback methods
- **TTS**: Voice selection, speech rate, audio quality

## ğŸ“š API Reference

### Main Endpoints

#### `POST /api/process`
Initiates video processing.

**Parameters:**
- `video` (file) or `video_url` (string): Video input
- `source_language` (string): Source language code
- `target_language` (string): Target language for summary
- `voice` (string): Voice ID for TTS
- `resolution` (string): Output resolution (480p, 720p, 1080p)
- `summary_length` (string): short, medium, long

**Response:**
```json
{
  "video_id": "unique-video-id",
  "status": "processing",
  "progress": 0,
  "message": "Processing started"
}
```

#### `GET /api/status/<video_id>`
Get processing status.

**Response:**
```json
{
  "status": "completed",
  "progress": 100,
  "message": "Processing complete!",
  "summaries": [...],
  "keywords": [...],
  "summary_videos": [...]
}
```

#### `GET /api/stream/<video_id>/<cluster_id>`
Stream summary video for a specific topic.

#### `GET /api/summary/<video_id>`
Get complete summary data including transcripts and metadata.

### Supported Languages

#### Indian Languages (22 Official Languages)
| Language | Code | Script | TTS Support | OCR Support |
|----------|------|--------|-------------|-------------|
| Hindi | `hi` | à¤¦à¥‡à¤µà¤¨à¤¾à¤—à¤°à¥€ | âœ… | âœ… |
| Bengali | `bn` | à¦¬à¦¾à¦‚à¦²à¦¾ | âœ… | âœ… |
| Telugu | `te` | à°¤à±†à°²à±à°—à± | âœ… | âœ… |
| Marathi | `mr` | à¤®à¤°à¤¾à¤ à¥€ | âœ… | âœ… |
| Tamil | `ta` | à®¤à®®à®¿à®´à¯ | âœ… | âœ… |
| Gujarati | `gu` | àª—à«àªœàª°àª¾àª¤à«€ | âœ… | âœ… |
| Urdu | `ur` | Ø§Ø±Ø¯Ùˆ | âœ… | âœ… |
| Kannada | `kn` | à²•à²¨à³à²¨à²¡ | âœ… | âœ… |
| Malayalam | `ml` | à´®à´²à´¯à´¾à´³à´‚ | âœ… | âœ… |
| Punjabi | `pa` | à¨ªà©°à¨œà¨¾à¨¬à©€ | âœ… | âœ… |
| Odia | `or` | à¬“à¬¡à¬¼à¬¿à¬† | âœ… | âœ… |
| Assamese | `as` | à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾ | âœ… | âœ… |
| Nepali | `ne` | à¤¨à¥‡à¤ªà¤¾à¤²à¥€ | âœ… | âœ… |
| Sanskrit | `sa` | à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤ | âœ… | âœ… |

#### International Languages
English, Arabic, Chinese, Spanish, French, German, Japanese, Korean, Russian, Portuguese, Italian, Dutch, Turkish, Polish, Thai, Vietnamese, Indonesian, Malay

## ğŸ”§ Advanced Usage

### Custom Model Integration

You can integrate custom models for specialized use cases:

```python
# Example: Custom summarization model
from utils.summarizer import load_summarizer

# Load custom model
custom_model = load_summarizer('your-custom-model-name')

# Use in processing pipeline
summary = custom_model.summarize(text, language='hindi')
```

### Batch Processing

For processing multiple videos:

```python
from utils import process_video_batch

videos = [
    {'path': 'video1.mp4', 'language': 'hindi'},
    {'path': 'video2.mp4', 'language': 'tamil'},
]

results = process_video_batch(videos, target_language='english')
```

### Custom Translation

Add support for additional translation services:

```python
from utils.translator import register_translation_method

def custom_translate(text, source, target):
    # Your custom translation logic
    return translated_text

register_translation_method('custom', custom_translate)
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest

# Run with coverage
pytest --cov=utils tests/

# Run specific test category
pytest tests/test_transcription.py -v
```

## ğŸ› Troubleshooting

### Common Issues

1. **FFmpeg not found**
   - Ensure FFmpeg is installed and in system PATH
   - Test with: `ffmpeg -version`

2. **Tesseract OCR errors**
   - Install language packs for your target languages
   - Verify installation: `tesseract --list-langs`

3. **Memory issues with large videos**
   - Reduce video resolution before processing
   - Use smaller Whisper models
   - Increase system swap space

4. **Translation API limits**
   - Switch to offline translation models
   - Implement API key rotation
   - Use rate limiting

5. **Slow processing**
   - Use GPU acceleration if available
   - Reduce frame extraction interval
   - Use smaller AI models

### Performance Optimization

- **GPU Support**: Install CUDA for faster processing
- **Model Caching**: Models are cached after first load
- **Parallel Processing**: Multiple videos can be processed simultaneously
- **Resource Management**: Automatic cleanup of temporary files

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Install development dependencies: `pip install -r requirements-dev.txt`
4. Make your changes and add tests
5. Run tests: `pytest`
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI Whisper** for state-of-the-art speech recognition
- **Hugging Face Transformers** for NLP models
- **Google Translate** for translation services
- **gTTS** for text-to-speech synthesis
- **OpenCV** for computer vision
- **Flask** for the web framework
- **The open-source community** for various libraries and tools

## ğŸ“Š Performance Metrics

| Video Length | Processing Time | Memory Usage | Accuracy |
|--------------|----------------|--------------|----------|
| 5 minutes    | ~2 minutes     | 2GB         | 95%      |
| 15 minutes   | ~6 minutes     | 4GB         | 94%      |
| 30 minutes   | ~12 minutes    | 6GB         | 93%      |
| 60 minutes   | ~25 minutes    | 8GB         | 92%      |

*Performance metrics are approximate and may vary based on hardware and content complexity.*

## ğŸ”® Future Enhancements

- [x] Dynamic topic extraction with optimal topic count
- [x] Meaningful topic naming based on content
- [x] Intelligent keyframe selection per topic
- [x] Enhanced audio-video synchronization
- [ ] Real-time video processing
- [ ] Multi-speaker identification
- [ ] Advanced emotion detection
- [ ] Custom model training interface
- [ ] Mobile app support
- [ ] Cloud deployment options
- [ ] Advanced analytics dashboard
- [ ] Video editing capabilities

## ğŸ“ Support

- **Documentation**: [Wiki](https://github.com/yourusername/video-summarizer/wiki)
- **Issues**: [GitHub Issues](https://github.com/yourusername/video-summarizer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/video-summarizer/discussions)
- **Email**: support@video-summarizer.com

---

Built with â¤ï¸ for the global community to make video content more accessible across languages and cultures.
</file>

<file path="SETUP.md">
# ğŸš€ Quick Setup Guide for AI Video Summarizer

This guide will help you set up the AI Video Summarizer quickly and efficiently.

## ğŸ“‹ Prerequisites Checklist

Before starting, ensure you have:

- [ ] Python 3.8 or higher
- [ ] At least 4GB RAM (8GB recommended)
- [ ] 10GB free disk space
- [ ] Internet connection for translation services
- [ ] Git installed

## ğŸ”§ System Dependencies

### Windows
```bash
# Install FFmpeg
# Download from https://ffmpeg.org/download.html
# Add to system PATH

# Install Tesseract OCR
# Download from https://github.com/UB-Mannheim/tesseract/wiki
# Install with all language packs
```

### macOS
```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install dependencies
brew install ffmpeg tesseract tesseract-lang
```

### Ubuntu/Debian
```bash
# Update package list
sudo apt update

# Install FFmpeg
sudo apt install ffmpeg

# Install Tesseract with Indian language support
sudo apt install tesseract-ocr tesseract-ocr-hin tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-tel tesseract-ocr-kan tesseract-ocr-mal tesseract-ocr-urd tesseract-ocr-guj tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-mar tesseract-ocr-nep tesseract-ocr-san

# Install additional dependencies
sudo apt install python3-dev python3-pip python3-venv build-essential
```

## ğŸ“¥ Installation Steps

### 1. Clone and Setup
```bash
# Clone the repository
git clone https://github.com/meghanavemala/major-project.git
cd major-project

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip
```

### 2. Install Python Dependencies
```bash
# Install all required packages
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('punkt_tab')"

# Test installation
python -c "import torch; import whisper; import cv2; print('âœ… All dependencies installed successfully!')"
```

### 3. Verify System Tools
```bash
# Test FFmpeg
ffmpeg -version

# Test Tesseract
tesseract --version
tesseract --list-langs

# Test Python imports
python -c "
import whisper
import transformers
import pytesseract
import easyocr
print('âœ… All AI models accessible!')
"
```

### 4. Create Configuration
```bash
# Create environment file
cat > .env << EOF
# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-change-this
FLASK_ENV=production

# Processing Limits
MAX_CONTENT_LENGTH=104857600
MAX_VIDEO_DURATION=3600

# OCR Configuration
ENABLE_OCR=true

# Model Settings
WHISPER_MODEL_SIZE=medium
TRANSLATION_METHOD=google
EOF
```

### 5. Test Run
```bash
# Start the application
python app.py

# Check if it's running
curl http://localhost:5000
```

## ğŸŒ Browser Setup

1. Open your browser
2. Navigate to `http://localhost:5000`
3. You should see the AI Video Summarizer interface

## ğŸ” Troubleshooting

### Common Issues and Solutions

#### âŒ "FFmpeg not found"
```bash
# Windows: Ensure FFmpeg is in PATH
where ffmpeg

# macOS/Linux: Check installation
which ffmpeg

# If not found, reinstall following system dependencies above
```

#### âŒ "No module named 'torch'"
```bash
# Reinstall PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```
```bash
# If want to use GPU
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### âŒ "Tesseract not found"
```bash
# Check Tesseract installation
tesseract --version

# If not found, install following system dependencies
# Then test Python integration:
python -c "import pytesseract; print(pytesseract.image_to_string('test.png'))"
```

#### âŒ "CUDA out of memory"
```bash
# Use CPU-only mode by setting in your environment:
export CUDA_VISIBLE_DEVICES=""
# Or modify model sizes in config to use smaller models
```

#### âŒ Translation errors
```bash
# Check internet connection
ping translate.googleapis.com

# Fallback to offline translation
# Edit app.py and set: TRANSLATION_METHOD=m2m100
```

#### âŒ Port already in use
```bash
# Check what's using port 5000
# Windows:
netstat -ano | findstr :5000
# macOS/Linux:
lsof -i :5000

# Kill the process or use different port:
python app.py --port 5001
```

## ğŸ§ª Test With Sample Video

1. Download a short test video:
```bash
# Create test directory
mkdir test_videos
cd test_videos

# Download a sample video (or use your own)
# Upload it through the web interface
```

2. Test basic functionality:
   - Upload the video
   - Select source language (or auto-detect)
   - Select target language
   - Choose voice and quality settings
   - Click "Process Video"

3. Expected results:
   - Processing progress should show
   - Topics should be generated
   - Summary videos should be playable
   - Downloads should work

## âš¡ Performance Optimization

### For Better Speed:
```bash
# Use GPU if available
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Reduce model sizes for faster processing
# Edit the configuration to use smaller models:
# WHISPER_MODEL_SIZE=base
# Use 480p resolution instead of 1080p
```

### For Better Quality:
```bash
# Use larger models (slower but more accurate)
# WHISPER_MODEL_SIZE=large
# Enable all OCR features
# Use 1080p resolution
```

## ğŸ”’ Production Deployment

### Security Setup:
```bash
# Generate secure secret key
python -c "import secrets; print(secrets.token_hex(32))"

# Update .env with the generated key
# Set FLASK_ENV=production
```

### Using Gunicorn:
```bash
# Install gunicorn
pip install gunicorn

# Run with gunicorn
gunicorn --workers 4 --bind 0.0.0.0:8000 app:app
```

### Using Docker:
```bash
# Build Docker image
docker build -t video-summarizer .

# Run container
docker run -p 5000:5000 -v $(pwd)/uploads:/app/uploads -v $(pwd)/processed:/app/processed video-summarizer
```

## ğŸ“Š Monitoring and Logs

### Check Application Logs:
```bash
# View recent logs
tail -f app.log

# Check for errors
grep ERROR app.log
```

### Monitor Performance:
```bash
# Check disk usage
df -h

# Check memory usage
free -h

# Monitor processing
ps aux | grep python
```

## ğŸ†˜ Getting Help

If you encounter issues:

1. **Check the logs**: Look at `app.log` for error messages
2. **Verify dependencies**: Ensure all system tools are installed
3. **Test components**: Run individual tests for each component
4. **Check GitHub Issues**: Look for similar problems
5. **Create an Issue**: Provide logs and system information

### System Information Template:
```bash
# Get system info for bug reports
echo "OS: $(uname -a)"
echo "Python: $(python --version)"
echo "FFmpeg: $(ffmpeg -version | head -1)"
echo "Tesseract: $(tesseract --version | head -1)"
echo "GPU: $(nvidia-smi | head -1 || echo 'No NVIDIA GPU')"
pip list | grep -E "(torch|whisper|opencv|transformers)"
```

## ğŸ‰ You're Ready!

Congratulations! Your AI Video Summarizer is now set up and ready to use. Start by uploading a short video to test all features.

### Next Steps:
- Try different languages
- Experiment with voice options
- Test topic selection
- Explore download features
- Customize settings for your needs

Happy summarizing! ğŸ¬âœ¨
</file>

<file path="static/style.css">
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    min-height: 100vh;
    color: #333;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

.header {
    text-align: center;
    margin-bottom: 40px;
    color: white;
}

.header h1 {
    font-size: 3.5rem;
    font-weight: 700;
    margin-bottom: 10px;
    text-shadow: 0 4px 20px rgba(0,0,0,0.3);
}

.header p {
    font-size: 1.2rem;
    opacity: 0.9;
    font-weight: 300;
}

.features {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 20px;
    margin-bottom: 40px;
}

.feature-card {
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(10px);
    border-radius: 20px;
    padding: 30px;
    text-align: center;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
    transition: transform 0.3s ease, box-shadow 0.3s ease;
}

.feature-card:hover {
    transform: translateY(-5px);
    box-shadow: 0 12px 40px rgba(0,0,0,0.15);
}

.feature-icon {
    font-size: 2.5rem;
    margin-bottom: 15px;
    background: linear-gradient(135deg, #667eea, #764ba2);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
}

.feature-card h3 {
    font-size: 1.3rem;
    font-weight: 600;
    margin-bottom: 10px;
    color: #333;
}

.feature-card p {
    color: #666;
    line-height: 1.6;
}

.upload-section {
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(10px);
    border-radius: 25px;
    padding: 40px;
    margin-bottom: 40px;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}

.upload-title {
    text-align: center;
    margin-bottom: 30px;
}

.upload-title h2 {
    font-size: 2rem;
    font-weight: 600;
    color: #333;
    margin-bottom: 10px;
}

.upload-title p {
    color: #666;
    font-size: 1.1rem;
}

.form-group {
    margin-bottom: 25px;
}

.form-group label {
    display: block;
    margin-bottom: 8px;
    font-weight: 500;
    color: #333;
    font-size: 1rem;
}

.form-control {
    width: 100%;
    padding: 15px;
    border: 2px solid #e1e5e9;
    border-radius: 12px;
    font-size: 1rem;
    transition: border-color 0.3s ease, box-shadow 0.3s ease;
    background: white;
}

.form-control:focus {
    outline: none;
    border-color: #667eea;
    box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
}

.form-row {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px;
}

.file-upload-area {
    border: 3px dashed #667eea;
    border-radius: 15px;
    padding: 40px;
    text-align: center;
    background: rgba(102, 126, 234, 0.05);
    transition: all 0.3s ease;
    cursor: pointer;
}

.file-upload-area:hover {
    border-color: #764ba2;
    background: rgba(118, 75, 162, 0.05);
}

.file-upload-area.dragover {
    border-color: #764ba2;
    background: rgba(118, 75, 162, 0.1);
    transform: scale(1.02);
}

.upload-icon {
    font-size: 3rem;
    color: #667eea;
    margin-bottom: 15px;
}

.upload-text {
    font-size: 1.2rem;
    color: #333;
    margin-bottom: 10px;
}

.upload-hint {
    color: #666;
    font-size: 0.9rem;
}

.btn {
    background: linear-gradient(135deg, #667eea, #764ba2);
    color: white;
    border: none;
    padding: 15px 30px;
    border-radius: 12px;
    font-size: 1.1rem;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
    text-decoration: none;
    display: inline-block;
    text-align: center;
}

.btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 8px 25px rgba(102, 126, 234, 0.3);
}

.btn:disabled {
    opacity: 0.6;
    cursor: not-allowed;
    transform: none;
}

.btn-secondary {
    background: linear-gradient(135deg, #6c757d, #495057);
}

.btn-secondary:hover {
    box-shadow: 0 8px 25px rgba(108, 117, 125, 0.3);
}

.processing-section {
    display: none;
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(10px);
    border-radius: 25px;
    padding: 40px;
    margin-bottom: 40px;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}

.processing-header {
    text-align: center;
    margin-bottom: 30px;
}

.processing-title {
    font-size: 2rem;
    font-weight: 600;
    color: #333;
    margin-bottom: 10px;
}

.progress-container {
    margin-bottom: 30px;
}

.progress-bar {
    width: 100%;
    height: 20px;
    background: #e1e5e9;
    border-radius: 10px;
    overflow: hidden;
    margin-bottom: 15px;
}

.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #667eea, #764ba2);
    border-radius: 10px;
    transition: width 0.5s ease;
    position: relative;
}

.progress-fill::after {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
    animation: shimmer 2s infinite;
}

@keyframes shimmer {
    0% { transform: translateX(-100%); }
    100% { transform: translateX(100%); }
}

.progress-text {
    text-align: center;
    font-size: 1.2rem;
    font-weight: 600;
    color: #333;
}

.progress-percentage {
    font-size: 2.5rem;
    font-weight: 700;
    color: #667eea;
    margin-bottom: 10px;
    text-align: center;
}

.progress-message {
    font-size: 1.1rem;
    color: #666;
    margin-bottom: 20px;
    text-align: center;
}

.steps-container {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 20px;
    margin-bottom: 30px;
}

.step {
    text-align: center;
    padding: 20px;
    border-radius: 15px;
    transition: all 0.3s ease;
}

.step.active {
    background: linear-gradient(135deg, #667eea, #764ba2);
    color: white;
    transform: scale(1.05);
}

.step.completed {
    background: linear-gradient(135deg, #28a745, #20c997);
    color: white;
}

.step.pending {
    background: #f8f9fa;
    color: #6c757d;
}

.step-icon {
    font-size: 2rem;
    margin-bottom: 10px;
}

.step-title {
    font-weight: 600;
    margin-bottom: 5px;
}

.step-description {
    font-size: 0.9rem;
    opacity: 0.8;
}

.results-section {
    display: none;
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(10px);
    border-radius: 25px;
    padding: 40px;
    margin-bottom: 40px;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}

.results-header {
    text-align: center;
    margin-bottom: 30px;
}

.results-title {
    font-size: 2rem;
    font-weight: 600;
    color: #333;
    margin-bottom: 10px;
}

.results-subtitle {
    color: #666;
    font-size: 1.1rem;
}

.topics-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 25px;
    margin-bottom: 40px;
}

.topic-card {
    background: white;
    border-radius: 20px;
    padding: 25px;
    box-shadow: 0 4px 20px rgba(0,0,0,0.1);
    transition: all 0.3s ease;
    cursor: pointer;
}

.topic-card:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 30px rgba(0,0,0,0.15);
}

.topic-card.active {
    border: 3px solid #667eea;
    transform: scale(1.02);
}

.topic-header {
    display: flex;
    align-items: center;
    margin-bottom: 15px;
}

.topic-number {
    background: linear-gradient(135deg, #667eea, #764ba2);
    color: white;
    width: 40px;
    height: 40px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: 600;
    margin-right: 15px;
}

.topic-title {
    font-size: 1.3rem;
    font-weight: 600;
    color: #333;
}

.topic-summary {
    color: #666;
    line-height: 1.6;
    margin-bottom: 15px;
}

.topic-keywords {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    margin-bottom: 15px;
}

.keyword-tag {
    background: rgba(102, 126, 234, 0.1);
    color: #667eea;
    padding: 5px 12px;
    border-radius: 20px;
    font-size: 0.9rem;
    font-weight: 500;
}

.topic-actions {
    display: flex;
    gap: 10px;
}

.btn-sm {
    padding: 8px 16px;
    font-size: 0.9rem;
}

.video-player-container {
    background: #000;
    border-radius: 20px;
    overflow: hidden;
    margin-bottom: 30px;
    box-shadow: 0 8px 30px rgba(0,0,0,0.3);
}

.video-player {
    width: 100%;
    height: auto;
    display: block;
}

.video-controls {
    background: rgba(0,0,0,0.8);
    padding: 20px;
    color: white;
}

.video-info {
    margin-bottom: 15px;
}

.video-title {
    font-size: 1.3rem;
    font-weight: 600;
    margin-bottom: 5px;
}

.video-description {
    color: #ccc;
    line-height: 1.5;
}

.download-section {
    text-align: center;
    margin-top: 30px;
}

.download-title {
    font-size: 1.5rem;
    font-weight: 600;
    color: #333;
    margin-bottom: 20px;
}

.download-buttons {
    display: flex;
    gap: 15px;
    justify-content: center;
    flex-wrap: wrap;
}

.error-message {
    background: linear-gradient(135deg, #dc3545, #c82333);
    color: white;
    padding: 20px;
    border-radius: 15px;
    text-align: center;
    margin-bottom: 20px;
    display: none;
}

.success-message {
    background: linear-gradient(135deg, #28a745, #20c997);
    color: white;
    padding: 20px;
    border-radius: 15px;
    text-align: center;
    margin-bottom: 20px;
    display: none;
}

.loading-spinner {
    display: inline-block;
    width: 40px;
    height: 40px;
    border: 4px solid #f3f3f3;
    border-top: 4px solid #667eea;
    border-radius: 50%;
    animation: spin 1s linear infinite;
    margin-right: 10px;
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

.hidden {
    display: none !important;
}

.text-center {
    text-align: center;
}

.checkbox-label {
    display: flex;
    align-items: center;
    cursor: pointer;
    font-weight: 500;
    color: #333;
}

.checkbox-label input[type="checkbox"] {
    margin-right: 10px;
    width: 18px;
    height: 18px;
}

@media (max-width: 768px) {
    .container {
        padding: 15px;
    }
    
    .header h1 {
        font-size: 2.5rem;
    }
    
    .form-row {
        grid-template-columns: 1fr;
    }
    
    .topics-grid {
        grid-template-columns: 1fr;
    }
    
    .download-buttons {
        flex-direction: column;
        align-items: center;
    }
}
</file>

<file path="templates/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Video Summarizer - Intelligent Video Processing</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="{{ url_for('static', filename='style.css') }}" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1><i class="fas fa-video"></i> AI Video Summarizer</h1>
            <p>Transform long videos into concise, intelligent summaries with multi-language support</p>
        </div>

        <!-- Features -->
        <div class="features">
            <div class="feature-card">
                <div class="feature-icon"><i class="fas fa-language"></i></div>
                <h3>22+ Indian Languages</h3>
                <p>Support for all official Indian languages with intelligent translation</p>
            </div>
            <div class="feature-card">
                <div class="feature-icon"><i class="fas fa-brain"></i></div>
                <h3>AI-Powered</h3>
                <p>Advanced AI models for transcription, summarization, and analysis</p>
            </div>
            <div class="feature-card">
                <div class="feature-icon"><i class="fas fa-layer-group"></i></div>
                <h3>Topic-Based</h3>
                <p>Intelligent topic clustering for organized content understanding</p>
            </div>
            <div class="feature-card">
                <div class="feature-icon"><i class="fas fa-microphone"></i></div>
                <h3>Voice Generation</h3>
                <p>Natural-sounding AI voice-overs in multiple languages</p>
            </div>
        </div>

        <!-- Upload Section -->
        <div class="upload-section" id="uploadSection">
            <div class="upload-title">
                <h2><i class="fas fa-upload"></i> Upload Your Video</h2>
                <p>Choose a video file or provide a YouTube URL to get started</p>
            </div>

            <form id="videoForm" enctype="multipart/form-data">
                <div class="form-row">
                <div class="form-group">
                        <label for="source_language"><i class="fas fa-language"></i> Source Language</label>
                        <select class="form-control" id="source_language" name="source_language" required>
                            <option value="auto">Auto-detect</option>
                            <option value="english">English</option>
                            <option value="hindi">Hindi</option>
                            <option value="bengali">Bengali</option>
                            <option value="telugu">Telugu</option>
                            <option value="marathi">Marathi</option>
                            <option value="tamil">Tamil</option>
                            <option value="urdu">Urdu</option>
                            <option value="gujarati">Gujarati</option>
                            <option value="kannada">Kannada</option>
                            <option value="odia">Odia</option>
                            <option value="punjabi">Punjabi</option>
                            <option value="malayalam">Malayalam</option>
                            <option value="assamese">Assamese</option>
                            <option value="maithili">Maithili</option>
                            <option value="santali">Santali</option>
                            <option value="kashmiri">Kashmiri</option>
                            <option value="nepali">Nepali</option>
                            <option value="konkani">Konkani</option>
                            <option value="dogri">Dogri</option>
                            <option value="manipuri">Manipuri</option>
                            <option value="bodo">Bodo</option>
                        </select>
                </div>
                <div class="form-group">
                        <label for="target_language"><i class="fas fa-flag"></i> Target Language</label>
                        <select class="form-control" id="target_language" name="target_language" required>
                            <option value="english">English</option>
                            <option value="hindi">Hindi</option>
                            <option value="bengali">Bengali</option>
                            <option value="telugu">Telugu</option>
                            <option value="marathi">Marathi</option>
                            <option value="tamil">Tamil</option>
                            <option value="urdu">Urdu</option>
                            <option value="gujarati">Gujarati</option>
                            <option value="kannada">Kannada</option>
                            <option value="odia">Odia</option>
                            <option value="punjabi">Punjabi</option>
                            <option value="malayalam">Malayalam</option>
                            <option value="assamese">Assamese</option>
                            <option value="maithili">Maithili</option>
                            <option value="santali">Santali</option>
                            <option value="kashmiri">Kashmiri</option>
                            <option value="nepali">Nepali</option>
                            <option value="konkani">Konkani</option>
                            <option value="dogri">Dogri</option>
                            <option value="manipuri">Manipuri</option>
                            <option value="bodo">Bodo</option>
                        </select>
                    </div>
                </div>
                
                <div class="form-row">
                    <div class="form-group">
                        <label for="voice"><i class="fas fa-microphone"></i> Voice</label>
                        <select class="form-control" id="voice" name="voice" required>
                            <option value="en-US-Standard-A">English - Female</option>
                            <option value="en-US-Standard-B">English - Male</option>
                            <option value="hi-IN-Standard-A">Hindi - Female</option>
                            <option value="hi-IN-Standard-B">Hindi - Male</option>
                            <option value="bn-IN-Standard-A">Bengali - Female</option>
                            <option value="bn-IN-Standard-B">Bengali - Male</option>
                            <option value="te-IN-Standard-A">Telugu - Female</option>
                            <option value="te-IN-Standard-B">Telugu - Male</option>
                        </select>
                    </div>
                <div class="form-group">
                        <label for="resolution"><i class="fas fa-expand"></i> Resolution</label>
                        <select class="form-control" id="resolution" name="resolution" required>
                            <option value="480p">480p</option>
                            <option value="720p">720p</option>
                            <option value="1080p">1080p</option>
                    </select>
                    </div>
                </div>
                
                <div class="form-group">
                    <label for="yt_url"><i class="fab fa-youtube"></i> YouTube URL (Optional)</label>
                    <input type="url" class="form-control" id="yt_url" name="yt_url" placeholder="https://www.youtube.com/watch?v=...">
                </div>
                
                <div class="form-group">
                    <label for="video_file"><i class="fas fa-file-video"></i> Or Upload Video File</label>
                    <div class="file-upload-area" id="fileUploadArea">
                        <div class="upload-icon"><i class="fas fa-cloud-upload-alt"></i></div>
                        <div class="upload-text">Click to upload or drag & drop</div>
                        <div class="upload-hint">MP4, AVI, MOV, MKV, WEBM (Max 40MB)</div>
                        <input type="file" id="video_file" name="video_file" accept="video/*" style="opacity: 0; position: absolute; pointer-events: none;">
                    </div>
                </div>
                
                <div class="form-group">
                    <label class="checkbox-label">
                        <input type="checkbox" id="enable_ocr" name="enable_ocr" checked>
                        <span class="checkmark"></span>
                        Enable OCR for better content understanding
                    </label>
                </div>
                
                <div class="form-group text-center">
                    <button type="submit" class="btn" id="submitBtn">
                        <i class="fas fa-play"></i> Start Processing
                    </button>
                    
                    <!-- Test Upload Button -->
                    <button type="button" id="testUploadBtn" class="btn btn-secondary" style="margin-left: 10px;">
                        <i class="fas fa-bug"></i> Test Upload
                    </button>
                </div>
            </form>
        </div>

        <!-- Processing Section -->
        <div class="processing-section" id="processingSection">
            <div class="processing-header">
                <h2 class="processing-title"><i class="fas fa-cogs"></i> Processing Your Video</h2>
            </div>

            <div class="progress-container">
                <div class="progress-percentage" id="progressPercentage">0%</div>
                <div class="progress-message" id="progressMessage">Initializing...</div>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill" style="width: 0%"></div>
                </div>
            </div>

            <div class="steps-container">
                <div class="step pending" id="step1">
                    <div class="step-icon"><i class="fas fa-download"></i></div>
                    <div class="step-title">Download/Upload</div>
                    <div class="step-description">Getting video file</div>
                </div>
                <div class="step pending" id="step2">
                    <div class="step-icon"><i class="fas fa-microphone"></i></div>
                    <div class="step-title">Transcription</div>
                    <div class="step-description">Converting speech to text</div>
                </div>
                <div class="step pending" id="step3">
                    <div class="step-icon"><i class="fas fa-image"></i></div>
                    <div class="step-title">Keyframes</div>
                    <div class="step-description">Extracting visual content</div>
                </div>
                <div class="step pending" id="step4">
                    <div class="step-icon"><i class="fas fa-brain"></i></div>
                    <div class="step-title">Analysis</div>
                    <div class="step-description">Clustering and summarizing</div>
                </div>
                <div class="step pending" id="step5">
                    <div class="step-icon"><i class="fas fa-video"></i></div>
                    <div class="step-title">Generation</div>
                    <div class="step-description">Creating summary videos</div>
                </div>
            </div>
        </div>

        <!-- Results Section -->
        <div class="results-section" id="resultsSection">
            <div class="results-header">
                <h2 class="results-title"><i class="fas fa-check-circle"></i> Processing Complete!</h2>
                <p class="results-subtitle">Your video has been successfully summarized into topics</p>
            </div>

            <div class="topics-grid" id="topicsGrid">
                <!-- Topics will be populated here -->
            </div>

            <div class="video-player-container" id="videoPlayerContainer">
                <video class="video-player" id="videoPlayer" controls>
                    <source src="" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="video-controls">
                    <div class="video-info">
                        <div class="video-title" id="videoTitle">Select a topic to view summary</div>
                        <div class="video-description" id="videoDescription">Click on any topic card above to start watching</div>
                    </div>
                </div>
            </div>

            <div class="download-section">
                <h3 class="download-title">Download Summary Videos</h3>
                <div class="download-buttons" id="downloadButtons">
                    <!-- Download buttons will be populated here -->
                </div>
            </div>
        </div>

        <!-- Messages -->
        <div class="error-message" id="errorMessage"></div>
        <div class="success-message" id="successMessage"></div>
    </div>

    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>
</file>

<file path="utils/clustering.py">
import logging
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
import re
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except LookupError:
    logger.info("Downloading NLTK data...")
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')

# Language-specific settings
LANGUAGE_STOPWORDS = {
    'en': set(stopwords.words('english')),
    'hi': set(stopwords.words('hindi') if 'hindi' in stopwords.fileids() else []),
    'kn': set(stopwords.words('kannada') if 'kannada' in stopwords.fileids() else [])
}

# Add custom stopwords for each language
CUSTOM_STOPWORDS = {
    'en': {'like', 'one', 'would', 'get', 'also', 'could', 'may', 'even', 'much', 'many', 'well'},
    'hi': set(),
    'kn': set()
}

# Combine NLTK and custom stopwords
for lang in LANGUAGE_STOPWORDS:
    LANGUAGE_STOPWORDS[lang].update(CUSTOM_STOPWORDS.get(lang, set()))

class TextPreprocessor:
    """Text preprocessing utilities for clustering."""
    
    def __init__(self, language: str = 'en'):
        """Initialize the text preprocessor.
        
        Args:
            language: Language code ('en', 'hi', 'kn')
        """
        self.language = language
        self.lemmatizer = WordNetLemmatizer()
        self.stopwords = LANGUAGE_STOPWORDS.get(language, set())
        
    def clean_text(self, text: str) -> str:
        """Clean and preprocess text."""
        if not isinstance(text, str):
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # Remove punctuation (language-specific handling)
        if self.language == 'en':
            # Keep sentence boundaries for English
            text = re.sub(r'[^\w\s.!?]', '', text)
        else:
            # For other languages, be more conservative with punctuation removal
            text = re.sub(r'[^\w\s]', ' ', text)
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """Tokenize text into words."""
        if not text.strip():
            return []
            
        # Tokenize
        tokens = word_tokenize(text, language='english' if self.language == 'en' else self.language)
        
        # Remove stopwords and short tokens
        tokens = [
            token for token in tokens 
            if token not in self.stopwords 
            and len(token) > 2
            and not token.isdigit()
        ]
        
        # Lemmatization (for English only)
        if self.language == 'en':
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        
        return tokens
    
    def preprocess(self, text: str) -> str:
        """Full preprocessing pipeline."""
        cleaned = self.clean_text(text)
        tokens = self.tokenize(cleaned)
        return ' '.join(tokens)

def cluster_segments(
    segments: List[Dict[str, Any]],
    n_clusters: int = 5,
    method: str = 'lda',
    language: str = 'en',
    min_topic_size: int = 2,
    max_features: int = 5000,
    min_cluster_size: int = 2,
    similarity_threshold: float = 0.7
) -> List[List[Dict[str, Any]]]:
    """Cluster text segments into topics.
    
    Args:
        segments: List of text segments with 'text' keys
        n_clusters: Number of clusters/topics to create
        method: Clustering method ('lda', 'nmf', 'kmeans', 'dbscan')
        language: Language code ('en', 'hi', 'kn')
        min_topic_size: Minimum number of segments per topic
        max_features: Maximum number of features for vectorization
        
    Returns:
        List of clusters, where each cluster is a list of segments
    """
    if not segments:
        return []
    
    # Validate language
    language = language.lower()
    if language not in ['en', 'hi', 'kn']:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    try:
        # Extract texts from segments
        texts = [seg.get('text', '') for seg in segments]
        
        # Initialize preprocessor
        preprocessor = TextPreprocessor(language)
        
        # Preprocess texts
        preprocessed_texts = [preprocessor.preprocess(text) for text in texts]
        
        # Vectorize texts
        if method in ['lda', 'nmf']:
            # For topic modeling, use count vectorizer
            vectorizer = CountVectorizer(
                max_features=max_features,
                stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
            )
            X = vectorizer.fit_transform(preprocessed_texts)
            
            # Apply topic modeling
            if method == 'lda':
                model = LatentDirichletAllocation(
                    n_components=n_clusters,
                    random_state=42,
                    learning_method='online',
                    max_iter=10
                )
                topic_assignments = model.fit_transform(X).argmax(axis=1)
            else:  # NMF
                model = NMF(
                    n_components=n_clusters,
                    random_state=42,
                    max_iter=1000
                )
                topic_assignments = model.fit_transform(X).argmax(axis=1)
                
        else:  # kmeans or dbscan
            # For clustering, use TF-IDF
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
            )
            X = vectorizer.fit_transform(preprocessed_texts)
            
            if method == 'kmeans':
                model = KMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    n_init=10
                )
                topic_assignments = model.fit_predict(X)
            else:  # dbscan
                model = DBSCAN(
                    eps=0.5,
                    min_samples=2,
                    metric='cosine'
                )
                topic_assignments = model.fit_predict(X)
                
                # Handle noise points (assigned to -1)
                if -1 in topic_assignments:
                    # Assign noise points to their own clusters
                    max_cluster = max(topic_assignments) + 1
                    topic_assignments = [x if x != -1 else max_cluster + i for i, x in enumerate(topic_assignments)]
        
        # Assign cluster labels to segments
        for i, seg in enumerate(segments):
            seg['cluster'] = int(topic_assignments[i])
        
        # Group segments by cluster
        clusters = defaultdict(list)
        for seg in segments:
            clusters[seg['cluster']].append(seg)
        
        # Filter small clusters
        filtered_clusters = [
            cluster for cluster in clusters.values() 
            if len(cluster) >= min_cluster_size
        ]
        
        # Sort clusters by size (largest first)
        filtered_clusters.sort(key=len, reverse=True)
        
        # Limit to n_clusters if we have more
        if len(filtered_clusters) > n_clusters:
            filtered_clusters = filtered_clusters[:n_clusters]
        
        logger.info(f"Created {len(filtered_clusters)} clusters with {sum(len(c) for c in filtered_clusters)} segments")
        
        return filtered_clusters
        
    except Exception as e:
        logger.error(f"Error in cluster_segments: {e}", exc_info=True)
        
        # Fallback: return all segments in a single cluster
        logger.warning("Falling back to single cluster")
        return [segments]

def extract_keywords(
    texts: List[str],
    n_keywords: int = 5,
    language: str = 'en',
    max_features: int = 5000
) -> List[str]:
    """Extract keywords from a list of texts."""
    if not texts:
        return []
    
    try:
        # Preprocess texts
        preprocessor = TextPreprocessor(language)
        preprocessed_texts = [' '.join(preprocessor.tokenize(text)) for text in texts]
        
        # Vectorize using TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
        )
        X = vectorizer.fit_transform(preprocessed_texts)
        
        # Get feature names (words)
        feature_names = vectorizer.get_feature_names_out()
        
        # Calculate average TF-IDF score for each word across all documents
        avg_tfidf = X.mean(axis=0).A1
        
        # Sort words by average TF-IDF score
        top_indices = avg_tfidf.argsort()[-n_keywords:][::-1]
        
        # Extract top keywords
        keywords = [feature_names[i] for i in top_indices if i < len(feature_names)]
        
        return keywords
    
    except Exception as e:
        logger.error(f"Error in extract_keywords: {e}", exc_info=True)
        return []
</file>

<file path="utils/summarizer.py">
import logging
from typing import List, Dict, Any, Optional
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import string

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
# try:
#     nltk.data.find('tokenizers/punkt')
#     nltk.data.find('corpora/stopwords')
# except LookupError:
#     logger.info("Downloading NLTK data...")
#     nltk.download('punkt')
#     nltk.download('stopwords')

# Initialize models as None. They will be loaded on first use.
SUMMARIZERS = {
    'en': None,  # English
    'hi': None,  # Hindi
    'kn': None   # Kannada
}

# Model configurations for different languages
MODEL_CONFIGS = {
    'en': {
        'model_name': 'facebook/bart-large-cnn',
        'min_length': 30,
        'max_length': 130,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    },
    'hi': {
        'model_name': 'csebuetnlp/mT5_multilingual_XLSum',
        'min_length': 30,
        'max_length': 100,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    },
    'kn': {
        'model_name': 'csebuetnlp/mT5_multilingual_XLSum',
        'min_length': 30,
        'max_length': 100,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    }
}

def preprocess_text(text: str, language: str = 'en') -> str:
    """Preprocess text before summarization."""
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Remove special characters and numbers
    if language == 'en':
        text = ''.join([char for char in text if char.isalnum() or char.isspace() or char in ',.!?'])
    
    return text

def extract_key_sentences(text: str, num_sentences: int = 3, language: str = 'en') -> List[str]:
    """Extract key sentences using a simple heuristic."""
    try:
        # Tokenize into sentences
        sentences = sent_tokenize(text, language='english' if language == 'en' else language)
        
        # Simple scoring based on word frequency
        words = [word.lower() for word in word_tokenize(text) 
                if word.lower() not in stopwords.words('english' if language == 'en' else language) 
                and word not in string.punctuation]
        
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Score sentences based on word frequency
        sentence_scores = {}
        for i, sentence in enumerate(sentences):
            for word in word_tokenize(sentence.lower()):
                if word in word_freq:
                    sentence_scores[i] = sentence_scores.get(i, 0) + word_freq[word]
        
        # Get top N sentences
        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
        top_sentences = sorted([i[0] for i in top_sentences])
        
        return [sentences[i] for i in top_sentences]
    except Exception as e:
        logger.warning(f"Error in extract_key_sentences: {e}")
        return text[:500].split('. ')[:3]  # Fallback: first few sentences

def load_summarizer(language: str = 'en') -> Any:
    """Load the appropriate summarization model for the specified language."""
    global SUMMARIZERS
    
    if language not in SUMMARIZERS:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    if SUMMARIZERS[language] is None:
        try:
            model_name = MODEL_CONFIGS[language]['model_name']
            logger.info(f"Loading {language} summarization model: {model_name}")
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"Using device: {device}")
            
            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
            
            # Create pipeline
            SUMMARIZERS[language] = {
                'pipeline': pipeline(
                    'summarization',
                    model=model,
                    tokenizer=tokenizer,
                    device=0 if device == 'cuda' else -1,
                    framework='pt'
                ),
                'config': MODEL_CONFIGS[language]
            }
            logger.info(f"{language.capitalize()} summarization model loaded successfully.")
            
        except Exception as e:
            logger.error(f"Error loading {language} summarization model: {e}")
            return None
    
    return SUMMARIZERS[language]

def summarize_cluster(
    cluster: List[Dict[str, Any]], 
    language: str = 'en',
    use_extractive: bool = False
) -> str:
    """
    Generate a summary for a cluster of text segments.
    
    Args:
        cluster: List of text segments with 'text' keys
        language: Language code ('en', 'hi', 'kn')
        use_extractive: Whether to use extractive summarization (faster but less coherent)
        
    Returns:
        Generated summary text
    """
    if not cluster:
        return "No content to summarize."
    
    # Combine all text from the cluster
    cluster_text = " ".join([seg.get('text', '') for seg in cluster])
    
    if not cluster_text.strip():
        return "No content to summarize."
    
    language = language.lower()
    if language not in ['en', 'hi', 'kn']:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    try:
        # Preprocess text
        cluster_text = preprocess_text(cluster_text, language)
        
        # For very short texts, just return as is
        if len(word_tokenize(cluster_text)) < 30:
            return cluster_text
        
        # Use extractive summarization for non-English or as a fallback
        if use_extractive or language in ['hi', 'kn']:
            key_sentences = extract_key_sentences(cluster_text, num_sentences=3, language=language)
            return ' '.join(key_sentences)
        
        # Use abstractive summarization for English
        summarizer = load_summarizer(language)
        if not summarizer:
            raise Exception(f"Failed to load {language} summarization model")
        
        # Get model config
        config = summarizer['config']
        
        # Split long text into chunks if needed (to avoid token limits)
        max_chunk_length = 1024 if language == 'en' else 768
        if len(cluster_text) > max_chunk_length:
            # Simple chunking by sentences
            sentences = sent_tokenize(cluster_text)
            chunks = []
            current_chunk = []
            current_length = 0
            
            for sent in sentences:
                sent_length = len(word_tokenize(sent))
                if current_length + sent_length > max_chunk_length and current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = []
                    current_length = 0
                current_chunk.append(sent)
                current_length += sent_length
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # Summarize each chunk
            chunk_summaries = []
            for chunk in chunks:
                summary = summarizer['pipeline'](
                    chunk,
                    min_length=config['min_length'],
                    max_length=config['max_length'],
                    repetition_penalty=config['repetition_penalty'],
                    length_penalty=config['length_penalty'],
                    num_beams=config['num_beams'],
                    truncation=True
                )
                chunk_summaries.append(summary[0]['summary_text'])
            
            # Combine chunk summaries
            combined_summary = ' '.join(chunk_summaries)
            
            # Final summary of combined chunk summaries if still too long
            if len(word_tokenize(combined_summary)) > 100:
                final_summary = summarizer['pipeline'](
                    combined_summary,
                    min_length=config['min_length'],
                    max_length=config['max_length'],
                    repetition_penalty=config['repetition_penalty'],
                    length_penalty=config['length_penalty'],
                    num_beams=config['num_beams'],
                    truncation=True
                )
                return final_summary[0]['summary_text']
            return combined_summary
        
        else:
            # Process in one go if text is short enough
            summary = summarizer['pipeline'](
                cluster_text,
                min_length=config['min_length'],
                max_length=config['max_length'],
                repetition_penalty=config['repetition_penalty'],
                length_penalty=config['length_penalty'],
                num_beams=config['num_beams'],
                truncation=True
            )
            return summary[0]['summary_text']
    
    except Exception as e:
        logger.error(f"Error in summarize_cluster: {e}", exc_info=True)
        # Fallback to extractive summarization
        try:
            key_sentences = extract_key_sentences(cluster_text, num_sentences=3, language=language)
            return ' '.join(key_sentences)
        except Exception as e2:
            logger.error(f"Fallback summarization also failed: {e2}")
            return cluster_text[:500] + "..."  # Return first 500 chars as fallback
</file>

<file path="utils/tts.py">
import os
import logging
from gtts import gTTS
from pydub import AudioSegment
from typing import Optional, Dict, Tuple
import tempfile
import time
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Language mappings for TTS
LANGUAGE_MAP = {
    # Major Indian Languages
    'hindi': 'hi',
    'bengali': 'bn',
    'telugu': 'te',
    'marathi': 'mr',
    'tamil': 'ta',
    'gujarati': 'gu',
    'urdu': 'ur',
    'kannada': 'kn',
    'odia': 'or',
    'malayalam': 'ml',
    'punjabi': 'pa',
    'assamese': 'as',
    'sanskrit': 'sa',
    
    # International languages
    'english': 'en',
    'arabic': 'ar',
    'chinese': 'zh',
    'spanish': 'es',
    'french': 'fr',
    'german': 'de',
    'japanese': 'ja',
    'korean': 'ko',
    'russian': 'ru'
}

# Supported voices configuration
SUPPORTED_VOICES = {
    'en': [
        {'id': 'en-US-Standard-A', 'name': 'English US (Female)'},
        {'id': 'en-US-Standard-B', 'name': 'English US (Male)'},
        {'id': 'en-GB-Standard-A', 'name': 'English UK (Female)'},
        {'id': 'en-GB-Standard-B', 'name': 'English UK (Male)'}
    ],
    'hi': [
        {'id': 'hi-IN-Standard-A', 'name': 'Hindi (Female)'},
        {'id': 'hi-IN-Standard-B', 'name': 'Hindi (Male)'}
    ],
    'bn': [
        {'id': 'bn-IN-Standard-A', 'name': 'Bengali (Female)'},
        {'id': 'bn-IN-Standard-B', 'name': 'Bengali (Male)'}
    ],
    'te': [
        {'id': 'te-IN-Standard-A', 'name': 'Telugu (Female)'},
        {'id': 'te-IN-Standard-B', 'name': 'Telugu (Male)'}
    ],
    'ta': [
        {'id': 'ta-IN-Standard-A', 'name': 'Tamil (Female)'},
        {'id': 'ta-IN-Standard-B', 'name': 'Tamil (Male)'}
    ]
}

# Voice settings for gTTS
VOICE_SETTINGS = {
    # Indian Languages
    'hi': {'tld': 'co.in', 'slow': False, 'lang': 'hi'},
    'bn': {'tld': 'co.in', 'slow': False, 'lang': 'bn'},
    'te': {'tld': 'co.in', 'slow': False, 'lang': 'te'},
    'mr': {'tld': 'co.in', 'slow': False, 'lang': 'mr'},
    'ta': {'tld': 'co.in', 'slow': False, 'lang': 'ta'},
    'gu': {'tld': 'co.in', 'slow': False, 'lang': 'gu'},
    'ur': {'tld': 'co.in', 'slow': False, 'lang': 'ur'},
    'kn': {'tld': 'co.in', 'slow': False, 'lang': 'kn'},
    'or': {'tld': 'co.in', 'slow': False, 'lang': 'or'},
    'ml': {'tld': 'co.in', 'slow': False, 'lang': 'ml'},
    'pa': {'tld': 'co.in', 'slow': False, 'lang': 'pa'},
    'as': {'tld': 'co.in', 'slow': False, 'lang': 'as'},
    'sa': {'tld': 'co.in', 'slow': True, 'lang': 'sa'},
    
    # International Languages
    'en': {'tld': 'com', 'slow': False, 'lang': 'en'},
    'ar': {'tld': 'com', 'slow': False, 'lang': 'ar'},
    'zh': {'tld': 'com', 'slow': False, 'lang': 'zh'},
    'es': {'tld': 'com', 'slow': False, 'lang': 'es'},
    'fr': {'tld': 'fr', 'slow': False, 'lang': 'fr'},
    'de': {'tld': 'de', 'slow': False, 'lang': 'de'},
    'ja': {'tld': 'co.jp', 'slow': False, 'lang': 'ja'},
    'ko': {'tld': 'co.kr', 'slow': False, 'lang': 'ko'},
    'ru': {'tld': 'ru', 'slow': False, 'lang': 'ru'}
}

def text_to_speech(
    text: str, 
    output_dir: str, 
    video_id: str, 
    cluster_id: int,
    voice: str = 'en-US-Standard-C',
    language: str = 'english',
    slow: bool = False,
    bitrate: str = '192k',
    max_retries: int = 3,
    retry_delay: int = 2
) -> Optional[str]:
    """Convert text to speech using gTTS with retry logic and offline fallback.
    
    Args:
        text: Text to convert to speech
        output_dir: Directory to save the output file
        video_id: Unique ID for the video
        cluster_id: ID of the current topic cluster
        voice: Voice ID to use (for compatibility with advanced TTS)
        language: Language of the text (supports all Indian languages)
        slow: Whether to speak slowly (better for some languages)
        bitrate: Audio bitrate (e.g., '128k', '192k', '256k')
        max_retries: Maximum number of retry attempts for network issues
        retry_delay: Delay in seconds between retries
        
    Returns:
        Relative path to the generated audio file, or None on failure
    """
    if not text.strip():
        logger.warning("Empty text provided for TTS")
        return None
    
    temp_mp3_path = None
    attempt = 0
    
    while attempt < max_retries:
        try:
            # Validate language
            language = language.lower()
            if language not in LANGUAGE_MAP:
                logger.warning(f"Unsupported language: {language}. Defaulting to English.")
                language = 'english'
                
            lang_code = LANGUAGE_MAP[language]
            voice_settings = VOICE_SETTINGS.get(lang_code, VOICE_SETTINGS['en']).copy()
            voice_settings['slow'] = slow
            
            logger.info(f"Generating speech in {language} (Attempt {attempt + 1}/{max_retries})...")
            
            # Create output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate a temporary file path
            with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_mp3:
                temp_mp3_path = temp_mp3.name
            
            # Generate speech using gTTS
            tts = gTTS(
                text=text,
                lang=voice_settings['lang'],
                tld=voice_settings['tld'],
                slow=voice_settings['slow']
            )
            
            # Save as MP3 first (gTTS works better with MP3)
            tts.save(temp_mp3_path)
            
            # Output file path (WAV format for better compatibility)
            output_file = os.path.join(output_dir, f"{video_id}_summary_{cluster_id}.wav")
            
            # Convert to WAV with pydub for better control over format
            audio = AudioSegment.from_mp3(temp_mp3_path)
            
            # Normalize audio volume
            audio = audio.normalize()
            
            # Export as WAV with specified bitrate
            audio.export(
                output_file,
                format='wav',
                parameters=['-ar', '44100', '-ac', '2', '-b:a', bitrate]
            )
            
            logger.info(f"Speech generated and saved to {output_file}")
            return output_file
            
        except Exception as e:
            attempt += 1
            logger.error(f"TTS attempt {attempt} failed: {str(e)}")
            
            if attempt < max_retries:
                logger.info(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                # Increase delay for next attempt
                retry_delay *= 2
            else:
                logger.error("All TTS attempts failed. Trying offline fallback...")
                # Here you could implement an offline TTS fallback
                # For now, we'll return None to indicate failure
                return None
        
        finally:
            # Clean up temporary files
            if temp_mp3_path and os.path.exists(temp_mp3_path):
                try:
                    os.remove(temp_mp3_path)
                except Exception as e:
                    logger.warning(f"Failed to remove temporary MP3 file: {e}")

def get_available_voices():
    """Get a dictionary of available voices organized by language.
    
    Returns:
        dict: A dictionary mapping language codes to lists of available voices
    """
    return SUPPORTED_VOICES

def get_available_languages():
    """Get a dictionary of available languages and their codes."""
    return LANGUAGE_MAP.copy()
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
# Environment variables
.env

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Project specific
uploads/*
!uploads/.gitkeep
processed/*
!processed/.gitkeep

# Logs
*.log

# Large files
*.mp4
*.mp3
*.wav
*.avi
*.mov
*.mkv

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
downloads/
zzz.py
wsgi.py
voice-cloning/
</file>

<file path="requirements.txt">
# ------------------------
# Core Web Framework
# ------------------------
Flask==2.3.3
Werkzeug>=2.3.0

# ------------------------
# AI / NLP / ML
# ------------------------
transformers==4.31.0
torch==2.0.1
torchaudio>=2.0.2
openai-whisper @ git+https://github.com/openai/whisper.git
scikit-learn==1.3.0
numpy==1.25.0
pandas==2.0.3

# ------------------------
# Audio / Speech Processing
# ------------------------
pydub==0.25.1
librosa==0.10.0
soundfile==0.12.1

# ------------------------
# Image / Video Processing
# ------------------------
opencv-python==4.8.0.76
Pillow==9.5.0
pytesseract==0.3.10

# ------------------------
# Clustering & Summarization
# ------------------------
nltk==3.8.1
sentence-transformers==2.2.2
spacy==3.7.2
yake==0.4.8
gensim==4.3.2

# ------------------------
# Translation
# ------------------------
deep-translator==1.11.4

# ------------------------
# Utilities
# ------------------------
tqdm==4.66.1
python-dotenv==1.0.0
requests==2.31.0

# ------------------------
# Video Generation / Editing
# ------------------------
moviepy==1.0.3
ffmpeg-python==0.2.0


yt-dlp
</file>

<file path="utils/translator.py">
"""
Language Translation Utility Module

This module provides comprehensive translation capabilities for the video summarizer,
enabling conversion between different Indian languages and international languages.
It supports both Google Translate API and offline translation models.

Author: Video Summarizer Team
Created: 2024
"""

import os
import logging
import json
from typing import Dict, List, Optional, Tuple, Union
# Optional: Google Translate client (used when available)
try:
    from googletrans import Translator, LANGUAGES  # type: ignore
    GOOGLETRANS_AVAILABLE = True
except Exception:
    # If googletrans is not installed or fails to import, we gracefully degrade
    GOOGLETRANS_AVAILABLE = False
    Translator = None  # type: ignore
    LANGUAGES = {}  # type: ignore
import torch
from transformers import (
    MarianMTModel, MarianTokenizer, 
    M2M100ForConditionalGeneration, M2M100Tokenizer,
    AutoTokenizer, AutoModelForSeq2SeqLM
)
import time
import pickle
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Language code mappings - comprehensive support for Indian languages
LANGUAGE_MAPPINGS = {
    # Indian Languages (official)
    'hindi': {'code': 'hi', 'google': 'hi', 'iso': 'hi', 'native': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€'},
    'bengali': {'code': 'bn', 'google': 'bn', 'iso': 'bn', 'native': 'à¦¬à¦¾à¦‚à¦²à¦¾'},
    'telugu': {'code': 'te', 'google': 'te', 'iso': 'te', 'native': 'à°¤à±†à°²à±à°—à±'},
    'marathi': {'code': 'mr', 'google': 'mr', 'iso': 'mr', 'native': 'à¤®à¤°à¤¾à¤ à¥€'},
    'tamil': {'code': 'ta', 'google': 'ta', 'iso': 'ta', 'native': 'à®¤à®®à®¿à®´à¯'},
    'gujarati': {'code': 'gu', 'google': 'gu', 'iso': 'gu', 'native': 'àª—à«àªœàª°àª¾àª¤à«€'},
    'urdu': {'code': 'ur', 'google': 'ur', 'iso': 'ur', 'native': 'Ø§Ø±Ø¯Ùˆ'},
    'kannada': {'code': 'kn', 'google': 'kn', 'iso': 'kn', 'native': 'à²•à²¨à³à²¨à²¡'},
    'odia': {'code': 'or', 'google': 'or', 'iso': 'or', 'native': 'à¬“à¬¡à¬¼à¬¿à¬†'},
    'malayalam': {'code': 'ml', 'google': 'ml', 'iso': 'ml', 'native': 'à´®à´²à´¯à´¾à´³à´‚'},
    'punjabi': {'code': 'pa', 'google': 'pa', 'iso': 'pa', 'native': 'à¨ªà©°à¨œà¨¾à¨¬à©€'},
    'assamese': {'code': 'as', 'google': 'as', 'iso': 'as', 'native': 'à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾'},
    'nepali': {'code': 'ne', 'google': 'ne', 'iso': 'ne', 'native': 'à¤¨à¥‡à¤ªà¤¾à¤²à¥€'},
    'sanskrit': {'code': 'sa', 'google': 'sa', 'iso': 'sa', 'native': 'à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤'},
    'sindhi': {'code': 'sd', 'google': 'sd', 'iso': 'sd', 'native': 'Ø³Ù†Ø¯Ú¾ÛŒ'},
    
    # International Languages
    'english': {'code': 'en', 'google': 'en', 'iso': 'en', 'native': 'English'},
    'arabic': {'code': 'ar', 'google': 'ar', 'iso': 'ar', 'native': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©'},
    'chinese': {'code': 'zh', 'google': 'zh', 'iso': 'zh', 'native': 'ä¸­æ–‡'},
    'spanish': {'code': 'es', 'google': 'es', 'iso': 'es', 'native': 'EspaÃ±ol'},
    'french': {'code': 'fr', 'google': 'fr', 'iso': 'fr', 'native': 'FranÃ§ais'},
    'german': {'code': 'de', 'google': 'de', 'iso': 'de', 'native': 'Deutsch'},
    'japanese': {'code': 'ja', 'google': 'ja', 'iso': 'ja', 'native': 'æ—¥æœ¬èª'},
    'korean': {'code': 'ko', 'google': 'ko', 'iso': 'ko', 'native': 'í•œêµ­ì–´'},
    'russian': {'code': 'ru', 'google': 'ru', 'iso': 'ru', 'native': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'},
    'portuguese': {'code': 'pt', 'google': 'pt', 'iso': 'pt', 'native': 'PortuguÃªs'},
}

# Translation model preferences - ordered by quality
TRANSLATION_METHODS = [
    'google_translate',  # Best quality, requires internet
    'm2m100',           # Good multilingual model, offline
    'marian',           # Good for specific language pairs, offline
    'fallback'          # Simple word replacement, last resort
]

# Cache for translation models to avoid reloading
translation_cache = {
    'google_translator': None,
    'm2m100_model': None,
    'm2m100_tokenizer': None,
    'marian_models': {},
    'cache_dir': 'translation_cache'
}

def ensure_cache_dir():
    """Ensure translation cache directory exists."""
    cache_dir = Path(translation_cache['cache_dir'])
    cache_dir.mkdir(exist_ok=True)
    return cache_dir

def get_google_translator():
    """Get or create Google Translator instance."""
    if not GOOGLETRANS_AVAILABLE:
        logger.warning("googletrans not installed; skipping Google Translate. Install with: pip install googletrans==4.0.0rc1")
        return None
    if translation_cache['google_translator'] is None:
        try:
            translation_cache['google_translator'] = Translator()
            logger.info("Google Translator initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Google Translator: {e}")
            return None
    return translation_cache['google_translator']

def get_m2m100_model():
    """Get or load M2M100 multilingual translation model."""
    if translation_cache['m2m100_model'] is None:
        try:
            logger.info("Loading M2M100 multilingual translation model...")
            model_name = "facebook/m2m100_418M"
            
            # Check if CUDA is available
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"Using device: {device}")
            
            # Load tokenizer and model
            tokenizer = M2M100Tokenizer.from_pretrained(model_name)
            model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)
            
            translation_cache['m2m100_tokenizer'] = tokenizer
            translation_cache['m2m100_model'] = model
            
            logger.info("M2M100 model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load M2M100 model: {e}")
            return None, None
    
    return translation_cache['m2m100_model'], translation_cache['m2m100_tokenizer']

def translate_with_google(text: str, source_lang: str, target_lang: str) -> Optional[str]:
    """
    Translate text using Google Translate API.
    
    Args:
        text: Text to translate
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Translated text or None if failed
    """
    if not GOOGLETRANS_AVAILABLE:
        return None
    try:
        translator = get_google_translator()
        if not translator:
            return None
        
        # Convert language codes to Google Translate format
        src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('google', source_lang)
        tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('google', target_lang)
        
        # Perform translation
        result = translator.translate(text, src=src_code, dest=tgt_code)
        translated_text = result.text
        
        logger.info(f"Google Translate: {source_lang} -> {target_lang} successful")
        return translated_text
        
    except Exception as e:
        logger.error(f"Google Translate failed: {e}")
        return None

def translate_with_m2m100(text: str, source_lang: str, target_lang: str) -> Optional[str]:
    """
    Translate text using M2M100 multilingual model.
    
    Args:
        text: Text to translate
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Translated text or None if failed
    """
    try:
        model, tokenizer = get_m2m100_model()
        if not model or not tokenizer:
            return None
        
        # Convert language codes to M2M100 format
        src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('code', source_lang)
        tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('code', target_lang)
        
        # Set source language
        tokenizer.src_lang = src_code
        
        # Tokenize input
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        # Move to same device as model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate translation
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs,
                forced_bos_token_id=tokenizer.get_lang_id(tgt_code),
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
        
        # Decode translation
        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        
        logger.info(f"M2M100 translation: {source_lang} -> {target_lang} successful")
        return translated_text
        
    except Exception as e:
        logger.error(f"M2M100 translation failed: {e}")
        return None

def translate_text(
    text: str, 
    source_lang: str, 
    target_lang: str,
    method: str = 'auto'
) -> Tuple[Optional[str], str]:
    """
    Translate text from source language to target language.
    
    Args:
        text: Text to translate
        source_lang: Source language (name or code)
        target_lang: Target language (name or code)
        method: Translation method ('auto', 'google', 'm2m100', 'marian')
        
    Returns:
        Tuple of (translated_text, method_used)
    """
    if not text or not text.strip():
        return text, 'no_translation_needed'
    
    # Normalize language names
    source_lang = source_lang.lower()
    target_lang = target_lang.lower()
    
    # Check if translation is needed
    src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('code', source_lang)
    tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('code', target_lang)
    
    if src_code == tgt_code:
        logger.info(f"No translation needed: {source_lang} == {target_lang}")
        return text, 'no_translation_needed'
    
    # Try translation methods in order of preference
    methods_to_try = [method] if method != 'auto' else TRANSLATION_METHODS
    
    for translation_method in methods_to_try:
        if translation_method == 'google_translate':
            result = translate_with_google(text, source_lang, target_lang)
            if result:
                return result, 'google_translate'
                
        elif translation_method == 'm2m100':
            result = translate_with_m2m100(text, source_lang, target_lang)
            if result:
                return result, 'm2m100'
                
        elif translation_method == 'fallback':
            # Simple fallback - return original text with warning
            logger.warning(f"All translation methods failed, returning original text")
            return text, 'fallback'
    
    # If all methods fail, return original text
    logger.error(f"Translation failed for {source_lang} -> {target_lang}")
    return text, 'failed'

from concurrent.futures import ThreadPoolExecutor, as_completed

def translate_segments(
    segments: List[Dict], 
    source_lang: str, 
    target_lang: str,
    method: str = 'auto',
    max_workers: int = 5   # number of parallel threads
) -> List[Dict]:
    """
    Translate text segments from source to target language with multithreading.

    Args:
        segments: List of segment dictionaries with 'text' field
        source_lang: Source language
        target_lang: Target language
        method: Translation method to use
        max_workers: Number of threads for parallel translation

    Returns:
        List of segments with translated text
    """
    if not segments:
        return segments
    
    logger.info(f"Translating {len(segments)} segments from {source_lang} to {target_lang}")

    translated_segments = [None] * len(segments)  # placeholder for results
    translation_stats = {'success': 0, 'failed': 0, 'methods': {}}

    def translate_one(index, segment):
        original_text = segment.get('text', '')
        if not original_text.strip():
            return index, segment.copy()

        translated_text, method_used = translate_text(
            original_text, source_lang, target_lang, method
        )

        # stats
        if method_used != 'failed':
            translation_stats['success'] += 1
            translation_stats['methods'][method_used] = translation_stats['methods'].get(method_used, 0) + 1
        else:
            translation_stats['failed'] += 1

        new_segment = segment.copy()
        new_segment['text'] = translated_text
        new_segment['original_text'] = original_text
        new_segment['translation_method'] = method_used

        return index, new_segment

    # Run translations in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(translate_one, i, seg) for i, seg in enumerate(segments)]
        for future in as_completed(futures):
            idx, translated_seg = future.result()
            translated_segments[idx] = translated_seg

    logger.info(f"Translation complete: {translation_stats['success']} successful, {translation_stats['failed']} failed")
    logger.info(f"Methods used: {translation_stats['methods']}")
    
    return translated_segments


# def translate_segments(
#     segments: List[Dict], 
#     source_lang: str, 
#     target_lang: str,
#     method: str = 'auto'
# ) -> List[Dict]:
#     """
#     Translate text segments from source to target language.
    
#     Args:
#         segments: List of segment dictionaries with 'text' field
#         source_lang: Source language
#         target_lang: Target language
#         method: Translation method to use
        
#     Returns:
#         List of segments with translated text
#     """
#     if not segments:
#         return segments
    
#     logger.info(f"Translating {len(segments)} segments from {source_lang} to {target_lang}")
    
#     translated_segments = []
#     translation_stats = {'success': 0, 'failed': 0, 'methods': {}}
    
#     for i, segment in enumerate(segments):
#         original_text = segment.get('text', '')
        
#         if not original_text.strip():
#             translated_segments.append(segment.copy())
#             continue
        
#         # Translate the text
#         translated_text, method_used = translate_text(
#             original_text, source_lang, target_lang, method
#         )
        
#         # Update statistics
#         if method_used not in ['failed']:
#             translation_stats['success'] += 1
#             translation_stats['methods'][method_used] = translation_stats['methods'].get(method_used, 0) + 1
#         else:
#             translation_stats['failed'] += 1
        
#         # Create new segment with translated text
#         new_segment = segment.copy()
#         new_segment['text'] = translated_text
#         new_segment['original_text'] = original_text
#         new_segment['translation_method'] = method_used
        
#         translated_segments.append(new_segment)
        
#         # Progress logging
#         if (i + 1) % 10 == 0:
#             logger.info(f"Translated {i + 1}/{len(segments)} segments")
    
#     # Log final statistics
#     logger.info(f"Translation complete: {translation_stats['success']} successful, {translation_stats['failed']} failed")
#     logger.info(f"Methods used: {translation_stats['methods']}")
    
#     return translated_segments

def get_available_languages() -> Dict[str, Dict]:
    """
    Get all available languages with their metadata.
    
    Returns:
        Dictionary mapping language names to their metadata
    """
    return LANGUAGE_MAPPINGS.copy()

def get_language_pairs() -> List[Tuple[str, str]]:
    """
    Get all possible language pairs for translation.
    
    Returns:
        List of (source, target) language pairs
    """
    languages = list(LANGUAGE_MAPPINGS.keys())
    pairs = []
    
    for source in languages:
        for target in languages:
            if source != target:
                pairs.append((source, target))
    
    return pairs

def detect_language(text: str) -> Optional[str]:
    """
    Detect the language of given text.
    
    Args:
        text: Text to analyze
        
    Returns:
        Detected language code or None if detection fails
    """
    try:
        translator = get_google_translator()
        if not translator:
            return None
        
        detection = translator.detect(text)
        detected_lang = detection.lang
        
        # Convert Google language code to our internal format
        for lang_name, lang_data in LANGUAGE_MAPPINGS.items():
            if lang_data['google'] == detected_lang:
                return lang_name
        
        return detected_lang
        
    except Exception as e:
        logger.error(f"Language detection failed: {e}")
        return None

# Cache management functions
def save_translation_cache(filepath: str = None):
    """Save translation cache to disk."""
    if not filepath:
        cache_dir = ensure_cache_dir()
        filepath = cache_dir / 'translation_cache.pkl'
    
    try:
        cache_data = {
            'language_mappings': LANGUAGE_MAPPINGS,
            'timestamp': time.time()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(cache_data, f)
        
        logger.info(f"Translation cache saved to {filepath}")
    except Exception as e:
        logger.error(f"Failed to save translation cache: {e}")

def load_translation_cache(filepath: str = None):
    """Load translation cache from disk."""
    if not filepath:
        cache_dir = ensure_cache_dir()
        filepath = cache_dir / 'translation_cache.pkl'
    
    try:
        if Path(filepath).exists():
            with open(filepath, 'rb') as f:
                cache_data = pickle.load(f)
            
            logger.info(f"Translation cache loaded from {filepath}")
            return cache_data
        else:
            logger.info("No translation cache found")
            return None
    except Exception as e:
        logger.error(f"Failed to load translation cache: {e}")
        return None

# Initialize cache on module import
ensure_cache_dir()
</file>

<file path="utils/video_maker.py">
"""
This file handles the creation of summary videos by combining extracted keyframes, generated audio summaries, and optional subtitles.
It now supports per-topic video generation: for each topic, only the keyframes whose timestamps fall within the topic's segment times are used, and each keyframe is shown for a proportional duration to match the summary audio.
"""
import os
import cv2
import numpy as np
from pydub import AudioSegment
import subprocess
import logging
import shutil
from typing import Optional, Tuple, Dict, List
from pathlib import Path

# Supported video resolutions (width, height)
SUPPORTED_RESOLUTIONS = {
    '360p': (640, 360),
    '480p': (854, 480),
    '720p': (1280, 720),
    '1080p': (1920, 1080)
}

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def resize_frame(frame: np.ndarray, target_width: int = 854) -> np.ndarray:
    """Resize frame to target width while maintaining aspect ratio."""
    height, width = frame.shape[:2]
    aspect_ratio = width / height
    target_height = int(target_width / aspect_ratio)
    return cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_LINEAR)

def add_subtitle_to_frame(frame: np.ndarray, text: str, position: Tuple[int, int] = (50, 50), 
                         font_scale: float = 1.0, color: Tuple[int, int, int] = (255, 255, 255),
                         thickness: int = 2, font_face: int = cv2.FONT_HERSHEY_SIMPLEX) -> np.ndarray:
    """Add subtitle text to a video frame."""
    frame = frame.copy()  # Create a copy to avoid modifying the original
    
    # Split text into multiple lines if too long
    max_width = frame.shape[1] - 100  # Leave margins
    words = text.split()
    lines = []
    current_line = []
    
    for word in words:
        current_line.append(word)
        (text_width, _), _ = cv2.getTextSize(' '.join(current_line), font_face, font_scale, thickness)
        if text_width > max_width:
            if len(current_line) > 1:
                current_line.pop()
                lines.append(' '.join(current_line))
                current_line = [word]
            else:
                lines.append(' '.join(current_line))
                current_line = []
    
    if current_line:
        lines.append(' '.join(current_line))
    
    # Draw each line
    x, y = position
    line_height = int(text_height * 1.5)
    
    for i, line in enumerate(lines):
        y_pos = y + i * line_height
        (text_width, text_height), _ = cv2.getTextSize(line, font_face, font_scale, thickness)
        
        # Add black background for better text visibility
        cv2.rectangle(frame, 
                     (x-5, y_pos - text_height - 5),
                     (x + text_width + 5, y_pos + 5),
                     (0, 0, 0), -1)
        
        # Add text
        cv2.putText(frame, line, (x, y_pos), font_face, font_scale, color, thickness, cv2.LINE_AA)
    
    return frame

def select_keyframes_for_topic(keyframe_metadata, topic_start, topic_end):
    """Select keyframes whose timestamps fall within the topic's segment times."""
    return [kf for kf in keyframe_metadata if topic_start <= kf['timestamp'] <= topic_end]

# Update make_summary_video to accept a list of keyframe filepaths and their timestamps
# Instead of reading all images from a directory, accept a list of filepaths (with timestamps)
def make_summary_video(
    keyframes: list,  # List of dicts with 'filepath' and 'timestamp'
    tts_audio_path: str,
    output_path: str,
    target_width: int = 854,
    fps: int = 30,
    transition_frames: int = 10,  # Number of frames for transitions
    add_fade_effect: bool = True,   # Whether to add fade transitions
    max_total_frames: int = 9000    # Hard cap: 5 min at 30fps
) -> Optional[str]:
    """
    Create a summary video for a topic from selected keyframes and audio with improved alignment.
    Adds defensive checks and logging to prevent hangs.
    """
    temp_video = None
    temp_audio = None
    try:
        logger.info(f"[make_summary_video] Starting video creation: {output_path}")
        if not keyframes:
            logger.error("No keyframes provided for topic video.")
            return None
        # Defensive: Check all keyframe files exist
        for kf in keyframes:
            if not os.path.exists(kf['filepath']):
                logger.error(f"Keyframe file missing: {kf['filepath']}")
                return None
        # Sort keyframes by timestamp if available
        keyframes_with_time = [kf for kf in keyframes if 'timestamp' in kf]
        if keyframes_with_time:
            keyframes = sorted(keyframes_with_time, key=lambda x: x['timestamp'])
            logger.info(f"Sorted {len(keyframes)} keyframes by timestamp")
        # Read the first image to get dimensions
        first_img = cv2.imread(keyframes[0]['filepath'])
        if first_img is None:
            logger.error(f"Failed to read image: {keyframes[0]['filepath']}")
            return None
        first_img = resize_frame(first_img, target_width)
        height, width = first_img.shape[:2]
        # Output paths
        temp_video = output_path + "_temp.avi"
        final_video = output_path + ".mp4"
        # Create video writer
        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        out = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))
        if not out.isOpened():
            logger.error("Failed to create video writer with XVID codec.")
            return None
        # Load and process audio
        if not os.path.exists(tts_audio_path):
            logger.error(f"Audio file not found: {tts_audio_path}")
            return None
        audio = AudioSegment.from_file(tts_audio_path)
        audio_duration = len(audio) / 1000.0  # seconds
        total_frames = int(audio_duration * fps)
        if total_frames > max_total_frames:
            logger.warning(f"Capping total frames from {total_frames} to {max_total_frames}")
            total_frames = max_total_frames
        # Calculate frame distribution for better alignment with audio
        n_keyframes = len(keyframes)
        available_frames = total_frames - (transition_frames * (n_keyframes - 1))
        if available_frames <= 0:
            transition_frames = max(1, (total_frames // n_keyframes) // 4)
            available_frames = total_frames - (transition_frames * (n_keyframes - 1))
        base_frames_per_keyframe = available_frames // n_keyframes
        remaining_frames = available_frames - (base_frames_per_keyframe * n_keyframes)
        keyframe_frames = []
        for i in range(n_keyframes):
            extra = 1 if i < remaining_frames else 0
            keyframe_frames.append(base_frames_per_keyframe + extra)
        logger.info(f"Audio duration: {audio_duration:.2f}s, Total frames: {total_frames}, Frames per keyframe: {base_frames_per_keyframe} (avg)")
        # Write frames with transitions
        prev_img = None
        frames_written = 0
        for i, kf in enumerate(keyframes):
            img = cv2.imread(kf['filepath'])
            if img is None:
                logger.warning(f"Skipping corrupted image: {kf['filepath']}")
                continue
            img = resize_frame(img, target_width)
            frames_for_this_image = keyframe_frames[i]
            # Add transition from previous image if available
            if prev_img is not None and add_fade_effect and transition_frames > 0:
                for t in range(transition_frames):
                    if frames_written >= max_total_frames:
                        logger.warning("Frame cap reached during transitions.")
                        break
                    alpha = t / transition_frames
                    blended = cv2.addWeighted(prev_img, 1 - alpha, img, alpha, 0)
                    out.write(blended)
                    frames_written += 1
                frames_for_this_image = max(1, frames_for_this_image - transition_frames)
            # Write the main image frames
            for _ in range(frames_for_this_image):
                if frames_written >= max_total_frames:
                    logger.warning("Frame cap reached during main image writing.")
                    break
                out.write(img)
                frames_written += 1
            prev_img = img.copy()
            if frames_written >= max_total_frames:
                break
        out.release()
        logger.info(f"[make_summary_video] Finished writing {frames_written} frames to {temp_video}")
        # Defensive: Check temp video and audio exist and are not empty
        if not os.path.exists(temp_video) or os.path.getsize(temp_video) < 1000:
            logger.error(f"Temp video file missing or too small: {temp_video}")
            return None
        temp_audio = output_path + "_audio.wav"
        audio.export(temp_audio, format="wav")
        if not os.path.exists(temp_audio) or os.path.getsize(temp_audio) < 1000:
            logger.error(f"Temp audio file missing or too small: {temp_audio}")
            return None
        # Use FFmpeg to mux .avi and .wav into .mp4 with timeout
        cmd = [
            'ffmpeg', '-y', '-i', temp_video, '-i', temp_audio,
            '-c:v', 'libx264', '-profile:v', 'main', '-preset', 'medium',
            '-crf', '23', '-pix_fmt', 'yuv420p', '-c:a', 'aac', '-b:a', '192k',
            '-shortest', '-movflags', '+faststart', final_video
        ]
        import subprocess
        try:
            logger.info(f"[make_summary_video] Running FFmpeg: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=120)
            logger.debug(f"FFmpeg output: {result.stdout}")
            if not os.path.exists(final_video) or os.path.getsize(final_video) < 1000:
                logger.error(f"Output video not created or too small: {final_video}")
                return None
            logger.info(f"Successfully created topic summary video: {final_video}")
            return final_video
        except subprocess.TimeoutExpired:
            logger.error("FFmpeg process timed out!")
            return None
        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg error: {e.stderr}")
            return None
    except Exception as e:
        logger.error(f"Error in make_summary_video: {str(e)}", exc_info=True)
        return None
    finally:
        for temp_file in [temp_video, temp_audio]:
            try:
                if temp_file and os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                logger.warning(f"Failed to remove temp file {temp_file}: {e}")
</file>

<file path="static/script.js">
/**
 * Enhanced Video Summarizer Frontend JavaScript
 * 
 * This script handles the complete frontend functionality for the AI Video Summarizer,
 * including form submission, progress tracking, topic selection, and result display.
 * 
 * Features:
 * - Real-time progress tracking with step indicators
 * - Dynamic topic selection interface
 * - Video player for topic summaries
 * - Language-aware interface updates
 * - Download functionality
 * - Error handling and user feedback
 * 
 * Author: Video Summarizer Team
 * Created: 2024
 */

// Global variables
let currentVideoId = null;
let progressInterval = null;
let currentTopicIndex = 0;

// DOM elements
const uploadSection = document.getElementById('uploadSection');
const processingSection = document.getElementById('processingSection');
const resultsSection = document.getElementById('resultsSection');
const videoForm = document.getElementById('videoForm');
const submitBtn = document.getElementById('submitBtn');
const progressPercentage = document.getElementById('progressPercentage');
const progressMessage = document.getElementById('progressMessage');
const progressFill = document.getElementById('progressFill');
const topicsGrid = document.getElementById('topicsGrid');
const videoPlayerContainer = document.getElementById('videoPlayerContainer');
const videoPlayer = document.getElementById('videoPlayer');
const videoTitle = document.getElementById('videoTitle');
const videoDescription = document.getElementById('videoDescription');
const downloadButtons = document.getElementById('downloadButtons');
const errorMessage = document.getElementById('errorMessage');
const successMessage = document.getElementById('successMessage');

// Step elements
const steps = {
    step1: document.getElementById('step1'),
    step2: document.getElementById('step2'),
    step3: document.getElementById('step3'),
    step4: document.getElementById('step4'),
    step5: document.getElementById('step5')
};

// Initialize the application
document.addEventListener('DOMContentLoaded', function() {
    initializeFileUpload();
    initializeForm();
});

// File upload functionality
function initializeFileUpload() {
    const fileUploadArea = document.getElementById('fileUploadArea');
    const fileInput = document.getElementById('video_file');

    fileUploadArea.addEventListener('click', () => fileInput.click());
    
    fileUploadArea.addEventListener('dragover', (e) => {
    e.preventDefault();
        fileUploadArea.classList.add('dragover');
    });
    
    fileUploadArea.addEventListener('dragleave', () => {
        fileUploadArea.classList.remove('dragover');
    });
    
    fileUploadArea.addEventListener('drop', (e) => {
        e.preventDefault();
        fileUploadArea.classList.remove('dragover');
        
        if (e.dataTransfer.files.length > 0) {
            fileInput.files = e.dataTransfer.files;
            updateFileUploadDisplay(e.dataTransfer.files[0]);
        }
    });
    
    fileInput.addEventListener('change', (e) => {
        if (e.target.files.length > 0) {
            console.log('File selected:', e.target.files[0].name, e.target.files[0].size);
            updateFileUploadDisplay(e.target.files[0]);
        }
    });
}

function updateFileUploadDisplay(file) {
    const fileUploadArea = document.getElementById('fileUploadArea');
    const uploadIcon = fileUploadArea.querySelector('.upload-icon');
    const uploadText = fileUploadArea.querySelector('.upload-text');
    const uploadHint = fileUploadArea.querySelector('.upload-hint');
    
    uploadIcon.innerHTML = '<i class="fas fa-file-video"></i>';
    uploadText.textContent = file.name;
    uploadHint.textContent = `Size: ${(file.size / (1024 * 1024)).toFixed(2)} MB`;
}

// Form submission
function initializeForm() {
    videoForm.addEventListener('submit', async function(e) {
    e.preventDefault();

        if (!validateForm()) {
            return;
        }
        
        await submitForm();
    });
    
    // Add test upload button event listener
    const testUploadBtn = document.getElementById('testUploadBtn');
    if (testUploadBtn) {
        testUploadBtn.addEventListener('click', testUpload);
    }
}

function validateForm() {
    const ytUrl = document.getElementById('yt_url').value.trim();
    const videoFile = document.getElementById('video_file').files[0];
    
    console.log('Validation - ytUrl:', ytUrl);
    console.log('Validation - videoFile:', videoFile);
    
    if (!ytUrl && !videoFile) {
        showError('Please provide either a YouTube URL or upload a video file.');
        return false;
    }
    
    if (videoFile && videoFile.size > 40 * 1024 * 1024) {
        showError('Video file size must be less than 40MB.');
        return false;
    }
    
    return true;
}

async function submitForm() {
    try {
        submitBtn.disabled = true;
        submitBtn.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Processing...';
        
        const formData = new FormData(videoForm);
        
        // Ensure the file is properly attached to FormData
        const videoFile = document.getElementById('video_file').files[0];
        if (videoFile) {
            // Remove any existing entry and add the file
            formData.delete('video_file');
            formData.append('video_file', videoFile);
            console.log('File attached to FormData:', videoFile.name, videoFile.size);
        } else {
            console.log('No file found in video_file input');
        }
        
        // Debug logging
        console.log('Form data contents:');
        for (let [key, value] of formData.entries()) {
            if (value instanceof File) {
                console.log(`${key}: File - ${value.name} (${value.size} bytes)`);
            } else {
                console.log(`${key}: ${value}`);
            }
        }
        
        const response = await fetch('/api/process', {
            method: 'POST',
            body: formData
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const result = await response.json();
        
        if (result.error) {
            throw new Error(result.error);
        }
        
        currentVideoId = result.video_id;
        console.log('Processing started with video ID:', currentVideoId);
        showProcessingSection();
        startProgressTracking();
        
    } catch (error) {
        console.error('Error submitting form:', error);
        showError(`Error: ${error.message}`);
        submitBtn.disabled = false;
        submitBtn.innerHTML = '<i class="fas fa-play"></i> Start Processing';
    }
}

// Test upload functionality
async function testUpload() {
    try {
        const testBtn = document.getElementById('testUploadBtn');
        testBtn.disabled = true;
        testBtn.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Testing...';
        
        const formData = new FormData();
        
        // Get the video file
        const videoFile = document.getElementById('video_file').files[0];
        if (!videoFile) {
            showError('Please select a video file first.');
            testBtn.disabled = false;
            testBtn.innerHTML = '<i class="fas fa-bug"></i> Test Upload';
            return;
        }
        
        // Add the file to FormData
        formData.append('video_file', videoFile);
        
        console.log('Test upload - File being sent:', videoFile.name, videoFile.size);
        
        const response = await fetch('/api/test-upload', {
            method: 'POST',
            body: formData
        });
        
        const result = await response.json();
        
        if (result.success) {
            showSuccess(`Test successful! File: ${result.filename}, Size: ${(result.size / 1024 / 1024).toFixed(2)} MB`);
            console.log('Test upload result:', result);
        } else {
            showError(`Test failed: ${result.error}`);
            console.error('Test upload error:', result);
        }
        
    } catch (error) {
        console.error('Error in test upload:', error);
        showError(`Test error: ${error.message}`);
    } finally {
        const testBtn = document.getElementById('testUploadBtn');
        testBtn.disabled = false;
        testBtn.innerHTML = '<i class="fas fa-bug"></i> Test Upload';
    }
}

// Progress tracking
function startProgressTracking() {
    if (progressInterval) {
        clearInterval(progressInterval);
    }
    
    progressInterval = setInterval(async () => {
        try {
            const response = await fetch(`/api/status/${currentVideoId}`);
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const status = await response.json();
            console.log('Progress update:', status);
            updateProgress(status);
            
            if (status.status === 'completed') {
                console.log('Processing completed, showing results');
                clearInterval(progressInterval);
                showResults(status);
            } else if (status.status === 'error') {
                console.error('Processing failed:', status.error);
                clearInterval(progressInterval);
                showError(`Processing failed: ${status.error}`);
                showUploadSection();
            }
            
        } catch (error) {
            console.error('Error checking status:', error);
        }
    }, 1000);
}

function updateProgress(status) {
    const progress = status.progress || 0;
    const message = status.message || 'Processing...';
    
    progressPercentage.textContent = `${progress}%`;
    progressMessage.textContent = message;
    progressFill.style.width = `${progress}%`;
    
    // Update step status based on both progress and message
    updateStepStatus(progress);
    updateStepStatusByStage(status);
}

function updateStepStatus(progress) {
    // Reset all steps
    Object.values(steps).forEach(step => {
        step.className = 'step pending';
    });
    
    // Update steps based on progress
    if (progress >= 5) {
        steps.step1.className = 'step completed';
    }
    if (progress >= 20) {
        steps.step2.className = 'step completed';
    }
    if (progress >= 40) {
        steps.step3.className = 'step completed';
    }
    if (progress >= 55) {
        steps.step4.className = 'step completed';
    }
    if (progress >= 80) {
        steps.step5.className = 'step active';
    }
    if (progress >= 100) {
        steps.step5.className = 'step completed';
    }
}

// Enhanced step status update based on processing stage
function updateStepStatusByStage(status) {
    const message = status.message || '';
    const progress = status.progress || 0;
    
    console.log('Updating step status by stage:', { message, progress });
    
    // Reset all steps
    Object.values(steps).forEach(step => {
        step.className = 'step pending';
    });
    
    // Update steps based on processing stage
    if (message.includes('Downloading') || message.includes('Uploading')) {
        steps.step1.className = 'step active';
        console.log('Step 1: Download/Upload - ACTIVE');
    } else if (message.includes('Transcribing')) {
        steps.step1.className = 'step completed';
        steps.step2.className = 'step active';
        console.log('Step 1: Download/Upload - COMPLETED');
        console.log('Step 2: Transcription - ACTIVE');
    } else if (message.includes('Extracting') || message.includes('key frames')) {
        steps.step1.className = 'step completed';
        steps.step2.className = 'step completed';
        steps.step3.className = 'step active';
        console.log('Step 1: Download/Upload - COMPLETED');
        console.log('Step 2: Transcription - COMPLETED');
        console.log('Step 3: Keyframes - ACTIVE');
    } else if (message.includes('Analyzing') || message.includes('Clustering') || message.includes('Generating summary')) {
        steps.step1.className = 'step completed';
        steps.step2.className = 'step completed';
        steps.step3.className = 'step completed';
        steps.step4.className = 'step active';
        console.log('Step 1: Download/Upload - COMPLETED');
        console.log('Step 2: Transcription - COMPLETED');
        console.log('Step 3: Keyframes - COMPLETED');
        console.log('Step 4: Analysis - ACTIVE');
    } else if (message.includes('Creating video') || message.includes('Rendering')) {
        steps.step1.className = 'step completed';
        steps.step2.className = 'step completed';
        steps.step3.className = 'step completed';
        steps.step4.className = 'step completed';
        steps.step5.className = 'step active';
        console.log('Step 1: Download/Upload - COMPLETED');
        console.log('Step 2: Transcription - COMPLETED');
        console.log('Step 3: Keyframes - COMPLETED');
        console.log('Step 4: Analysis - COMPLETED');
        console.log('Step 5: Video Generation - ACTIVE');
    } else if (message.includes('complete') || message.includes('Complete')) {
        Object.values(steps).forEach(step => {
            step.className = 'step completed';
        });
        console.log('All steps completed!');
    }
}

// Section visibility
function showProcessingSection() {
    uploadSection.style.display = 'none';
    processingSection.style.display = 'block';
    resultsSection.style.display = 'none';
    hideMessages();
}

function showResults(data) {
    console.log('Showing results with data:', data);
    
    uploadSection.style.display = 'none';
    processingSection.style.display = 'none';
    resultsSection.style.display = 'block';
    
    // Store video data globally for access in other functions
    window.currentVideoData = data;
    
    // Check if we have the required data
    if (data.summaries && data.summaries.length > 0) {
        populateTopics(data);
        populateDownloadButtons(data);
        showSuccess('Video processing completed successfully!');
                } else {
        console.error('No summaries found in data:', data);
        showError('Processing completed but no summaries were generated.');
    }
}

function showUploadSection() {
    uploadSection.style.display = 'block';
    processingSection.style.display = 'none';
    resultsSection.style.display = 'none';
    submitBtn.disabled = false;
    submitBtn.innerHTML = '<i class="fas fa-play"></i> Start Processing';
}

// Populate results
function populateTopics(data) {
    topicsGrid.innerHTML = '';
    
    if (!data.summaries || data.summaries.length === 0) {
        topicsGrid.innerHTML = '<p class="text-center">No topics found.</p>';
        return;
    }
    
    data.summaries.forEach((summary, index) => {
        const topicCard = createTopicCard(summary, data.keywords[index], index, data);
        topicsGrid.appendChild(topicCard);
    });
}

function createTopicCard(summary, keywords, index, data) {
    const card = document.createElement('div');
    card.className = 'topic-card';
    card.dataset.topicIndex = index;
    
    const keywordsHtml = keywords ? keywords.map(kw => `<span class="keyword-tag">${kw}</span>`).join('') : '';
    
    card.innerHTML = `
        <div class="topic-header">
            <div class="topic-number">${index + 1}</div>
            <div class="topic-title">Topic ${index + 1}</div>
        </div>
        <div class="topic-summary">${summary}</div>
        <div class="topic-keywords">${keywordsHtml}</div>
        <div class="topic-actions">
            <button class="btn btn-sm" onclick="playTopic(${index})">
                <i class="fas fa-play"></i> Play Summary
            </button>
            <button class="btn btn-sm btn-secondary" onclick="downloadTopic(${index})">
                <i class="fas fa-download"></i> Download
            </button>
        </div>
    `;
    
    card.addEventListener('click', (e) => {
        if (!e.target.closest('button')) {
            playTopic(index);
        }
    });
    
    return card;
}

function populateDownloadButtons(data) {
    downloadButtons.innerHTML = '';
    
    if (!data.summary_videos || data.summary_videos.length === 0) {
        downloadButtons.innerHTML = '<p>No videos available for download.</p>';
        return;
    }
    
    data.summary_videos.forEach((videoPath, index) => {
        if (videoPath) {
            const downloadBtn = document.createElement('a');
            downloadBtn.href = `/processed/${videoPath}`;
            downloadBtn.className = 'btn btn-secondary';
            downloadBtn.download = `topic_${index + 1}_summary.mp4`;
            downloadBtn.innerHTML = `<i class="fas fa-download"></i> Topic ${index + 1}`;
            downloadButtons.appendChild(downloadBtn);
        }
    });
}

// Topic interaction
function playTopic(topicIndex) {
    // Update active topic card
    document.querySelectorAll('.topic-card').forEach(card => {
        card.classList.remove('active');
    });
    
    const activeCard = document.querySelector(`[data-topic-index="${topicIndex}"]`);
    if (activeCard) {
        activeCard.classList.add('active');
    }
    
    currentTopicIndex = topicIndex;
    
    // Check if video exists for this topic
    if (!window.currentVideoData || !window.currentVideoData.summary_videos || !window.currentVideoData.summary_videos[topicIndex]) {
        showError('Video for this topic is not available.');
        return;
    }
    
    // Update video player with organized path
    const videoPath = `/processed/${window.currentVideoData.summary_videos[topicIndex]}`;
    videoPlayer.src = videoPath;
    
    // Update video info with topic details
    const topicData = window.currentVideoData.clusters?.[topicIndex];
    if (topicData) {
        videoTitle.textContent = `Topic ${topicIndex + 1}: ${topicData.keywords?.slice(0, 3).join(', ') || 'Summary'}`;
        videoDescription.textContent = topicData.summary || 'Click play to start watching the summary video.';
    } else {
        videoTitle.textContent = `Topic ${topicIndex + 1} Summary`;
        videoDescription.textContent = 'Click play to start watching the summary video.';
    }
    
    // Show video player
    videoPlayerContainer.style.display = 'block';
    
    // Load video metadata
    videoPlayer.load();
}

function downloadTopic(topicIndex) {
    if (!window.currentVideoData || !window.currentVideoData.summary_videos || !window.currentVideoData.summary_videos[topicIndex]) {
        showError('Video for this topic is not available for download.');
        return;
    }
    
    const videoPath = `/processed/${window.currentVideoData.summary_videos[topicIndex]}`;
    const link = document.createElement('a');
    link.href = videoPath;
    link.download = `topic_${topicIndex + 1}_summary.mp4`;
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    
    showSuccess(`Downloading topic ${topicIndex + 1} summary video...`);
}

// Utility functions
function showError(message) {
    errorMessage.textContent = message;
    errorMessage.style.display = 'block';
    setTimeout(() => {
        errorMessage.style.display = 'none';
    }, 5000);
}

function showSuccess(message) {
    successMessage.textContent = message;
    successMessage.style.display = 'block';
    setTimeout(() => {
        successMessage.style.display = 'none';
    }, 5000);
}

function hideMessages() {
    errorMessage.style.display = 'none';
    successMessage.style.display = 'none';
}

// Reset functionality
function resetForm() {
    videoForm.reset();
    document.getElementById('fileUploadArea').innerHTML = `
        <div class="upload-icon"><i class="fas fa-cloud-upload-alt"></i></div>
        <div class="upload-text">Click to upload or drag & drop</div>
        <div class="upload-hint">MP4, AVI, MOV, MKV, WEBM (Max 40MB)</div>
    `;
    showUploadSection();
}

// Add reset button functionality
document.addEventListener('DOMContentLoaded', function() {
    // Add reset button to the form
    const resetBtn = document.createElement('button');
    resetBtn.type = 'button';
    resetBtn.className = 'btn btn-secondary';
    resetBtn.innerHTML = '<i class="fas fa-redo"></i> Reset';
    resetBtn.onclick = resetForm;
    
    const submitGroup = submitBtn.parentElement;
    submitGroup.appendChild(resetBtn);
});
</file>

<file path="utils/downloader.py">
import os
import re
import uuid
from urllib.parse import urlparse
import yt_dlp
import subprocess


def is_youtube_url(url: str) -> bool:
    """
    Check if the given string is a valid YouTube URL.
    """
    youtube_regex = (
        r'(https?://)?(www\.)?'
        r'(youtube|youtu|youtube-nocookie)\.(com|be)/'
        r'(watch\?v=|embed/|v/|.+/)?([\w-]{11})(?:\?[\w-]*=[\w-]*(?:&[\w-]*=[\w-]*)*)?$'
    )
    youtube_regex_match = re.match(youtube_regex, url)
    return youtube_regex_match is not None


def handle_video_upload_or_download(request, upload_dir):
    """Handle video upload or YouTube download with proper error handling."""
    import logging
    logger = logging.getLogger(__name__)

    try:
        # Ensure upload directory exists
        if not os.path.exists(upload_dir):
            os.makedirs(upload_dir, exist_ok=True)
            logger.info(f"Created upload directory: {upload_dir}")
        if not os.access(upload_dir, os.W_OK):
            raise ValueError(f"Upload directory is not writable: {upload_dir}")

        video_id = str(uuid.uuid4())
        logger.info(f"Processing video with ID: {video_id}")

        # Check for uploaded video file
        if 'video_file' in request.files and request.files['video_file']:
            f = request.files['video_file']
            if not f or not hasattr(f, 'filename'):
                raise ValueError("Invalid file object received")
            if not f.filename:
                raise ValueError("Uploaded file has no filename")

            logger.info(f"File object valid: {f.filename}, content_type: {getattr(f, 'content_type', 'unknown')}")

            # Get file size
            file_size = getattr(f, 'content_length', 0)
            if file_size > 100 * 1024 * 1024:
                raise ValueError(f"File too large: {file_size / (1024*1024):.1f}MB (max 100MB)")

            path = os.path.join(upload_dir, f"{video_id}_{f.filename}")
            with open(path, 'wb') as out_file:
                while True:
                    chunk = f.read(8192)
                    if not chunk:
                        break
                    out_file.write(chunk)

            actual_file_size = os.path.getsize(path)
            if actual_file_size > 100 * 1024 * 1024:
                os.remove(path)
                raise ValueError(f"File too large: {actual_file_size / (1024*1024):.1f}MB (max 100MB)")

            # Validate video with OpenCV
            try:
                import cv2
                cap = cv2.VideoCapture(path)
                if not cap.isOpened() or not cap.read()[0]:
                    raise ValueError("Invalid video file")
                cap.release()
            except ImportError:
                logger.warning("OpenCV not available, skipping video validation")

            return path, video_id

        # YouTube URL case
        elif 'yt_url' in request.form and request.form['yt_url'].strip():
            yt_url = request.form['yt_url'].strip()
            if not is_youtube_url(yt_url):
                raise ValueError("Invalid YouTube URL provided")

            temp_file = f"temp_video_{video_id}.mp4"
            ydl_opts = {
                'outtmpl': temp_file,
                'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',
                'merge_output_format': 'mp4'
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([yt_url])

            compressed_path = os.path.join(upload_dir, f"{video_id}.mp4")
            result = subprocess.run([
                "ffmpeg", "-i", temp_file,
                "-vcodec", "libx264", "-crf", "28",
                "-preset", "fast",
                "-acodec", "aac", "-b:a", "96k",
                compressed_path
            ], capture_output=True, text=True)

            if result.returncode != 0:
                raise RuntimeError(f"FFmpeg compression failed: {result.stderr}")

            if os.path.exists(temp_file):
                os.remove(temp_file)

            return compressed_path, video_id

        else:
            raise ValueError("No video file uploaded and no YouTube URL provided")

    except Exception as e:
        logger.error(f"Error in handle_video_upload_or_download: {str(e)}")
        raise


def handle_video_upload_or_download_from_data(video_file_data, yt_url_data, upload_dir):
    """Handle video upload or YouTube download using pre-copied data."""
    import logging
    logger = logging.getLogger(__name__)

    try:
        if not os.path.exists(upload_dir):
            os.makedirs(upload_dir, exist_ok=True)

        if not os.access(upload_dir, os.W_OK):
            raise ValueError(f"Upload directory is not writable: {upload_dir}")

        video_id = str(uuid.uuid4())

        if video_file_data and video_file_data.get('content'):
            filename = video_file_data['filename']
            content = video_file_data['content']
            path = os.path.join(upload_dir, f"{video_id}_{filename}")

            with open(path, 'wb') as out_file:
                out_file.write(content)
                out_file.flush()
                os.fsync(out_file.fileno())

            if os.path.getsize(path) != len(content):
                raise ValueError("File size mismatch")

            return path, video_id

        elif yt_url_data and yt_url_data.strip():
            yt_url = yt_url_data.strip()
            if not is_youtube_url(yt_url):
                raise ValueError("Invalid YouTube URL provided")

            temp_file = f"temp_video_{video_id}.mp4"
            ydl_opts = {
                'outtmpl': temp_file,
                'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',
                'merge_output_format': 'mp4'
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([yt_url])

            compressed_path = os.path.join(upload_dir, f"{video_id}.mp4")
            result = subprocess.run([
                "ffmpeg", "-i", temp_file,
                "-vcodec", "libx264", "-crf", "28",
                "-preset", "fast",
                "-acodec", "aac", "-b:a", "96k",
                compressed_path
            ], capture_output=True, text=True)

            if result.returncode != 0:
                raise RuntimeError(f"FFmpeg compression failed: {result.stderr}")

            if os.path.exists(temp_file):
                os.remove(temp_file)

            return compressed_path, video_id

        else:
            raise ValueError("No video file data or YouTube URL provided")

    except Exception as e:
        logger.error(f"Error in handle_video_upload_or_download_from_data: {str(e)}")
        raise


def handle_youtube_download(yt_url, upload_dir, video_id=None, status_callback=None):
    """Handle YouTube download only with real-time progress updates."""
    import logging, traceback
    logger = logging.getLogger(__name__)

    try:
        if not os.path.exists(upload_dir):
            os.makedirs(upload_dir, exist_ok=True)

        if not os.access(upload_dir, os.W_OK):
            raise ValueError(f"Upload directory is not writable: {upload_dir}")

        # Use provided video_id or generate new one
        if not video_id:
            video_id = str(uuid.uuid4())
        
        if not yt_url or not yt_url.strip():
            raise ValueError("No YouTube URL provided")

        yt_url = yt_url.strip()
        if not is_youtube_url(yt_url):
            raise ValueError("Invalid YouTube URL provided")

        temp_file = f"temp_video_{video_id}.mp4"
        
        # Progress hook function to update status in real-time
        def progress_hook(d):
            if status_callback and callable(status_callback):
                if d['status'] == 'downloading':
                    # Extract percentage from yt-dlp progress info
                    if '_percent_str' in d:
                        percent_str = d['_percent_str'].strip()
                        try:
                            # Convert "13.6%" to float
                            percent = float(percent_str.replace('%', ''))
                            # Map download progress to 5-15% range
                            progress = 5 + (percent * 0.1)  # 5% to 15%
                            status_callback('downloading', int(progress), f"Downloading video... {percent_str}")
                        except (ValueError, AttributeError):
                            # Fallback if percentage parsing fails
                            status_callback('downloading', 5, 'Downloading video...')
                    else:
                        status_callback('downloading', 5, 'Downloading video...')
                        
                elif d['status'] == 'finished':
                    status_callback('downloading', 15, 'Download complete, processing...')
                elif d['status'] == 'error':
                    status_callback('error', 0, f'Download failed: {d.get("error", "Unknown error")}')

        ydl_opts = {
            'outtmpl': temp_file,
            'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',
            'merge_output_format': 'mp4',
            'socket_timeout': 60,
            'progress_hooks': [progress_hook]
        }

        logger.info(f"Starting YouTube download with progress hooks...")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([yt_url])

        # Update status to show compression starting
        if status_callback:
            status_callback('downloading', 16, 'Compressing video...')

        compressed_path = os.path.join(upload_dir, f"{video_id}.mp4")
        result = subprocess.run([
            "ffmpeg", "-i", temp_file,
            "-vcodec", "libx264", "-crf", "28",
            "-preset", "fast",
            "-acodec", "aac", "-b:a", "96k",
            compressed_path
        ], capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"FFmpeg compression failed: {result.stderr}")

        if os.path.exists(temp_file):
            os.remove(temp_file)

        # Update status to show compression complete
        if status_callback:
            status_callback('downloading', 18, 'Video ready for processing')

        logger.info(f"YouTube video downloaded and compressed successfully: {compressed_path}")
        return compressed_path, video_id

    except Exception as e:
        logger.error(f"Error in handle_youtube_download: {str(e)}\n{traceback.format_exc()}")
        # Update status to show error
        if status_callback:
            status_callback('error', 0, f'Download failed: {str(e)}')
        raise
</file>

<file path="utils/keyframes.py">
"""
Enhanced Keyframe Extraction and OCR Analysis Module

This module now extracts more keyframes (lower frame_interval, higher max_keyframes) and stores timestamps for each keyframe,
enabling per-topic keyframe selection for summary video generation.
"""

import os
import cv2
import numpy as np
from typing import List, Optional, Dict, Any
import logging
import json
from collections import Counter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OCR Dependencies
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

try:
    from PIL import Image  # noqa: F401  (kept for potential future use)
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# OCR Configuration for different languages
OCR_LANGUAGES = {
    'english': 'eng',
    'hindi': 'hin',
    'bengali': 'ben',
    'telugu': 'tel',
    'marathi': 'mar',
    'tamil': 'tam',
    'gujarati': 'guj',
    'urdu': 'urd',
    'kannada': 'kan',
    'odia': 'ori',
    'malayalam': 'mal',
    'punjabi': 'pan',
    'assamese': 'asm',
    'nepali': 'nep',
    'sanskrit': 'san',
}

# EasyOCR language codes
EASYOCR_LANGUAGES = {
    'english': 'en',
    'hindi': 'hi',
    'bengali': 'bn',
    'tamil': 'ta',
    'korean': 'ko',
    'chinese': 'ch_sim',
    'japanese': 'ja',
    'arabic': 'ar',
}


class KeyframeOCR:
    """
    OCR engine for extracting text from keyframes.
    Supports multiple OCR backends with language-specific optimization.
    """

    def __init__(self, languages: List[str] = ['english'], ocr_engine: str = 'auto'):
        """
        Initialize OCR engine.

        Args:
            languages: List of languages to detect
            ocr_engine: OCR engine to use ('tesseract', 'easyocr', 'auto')
        """
        self.languages = languages
        self.ocr_engine = ocr_engine
        self.easyocr_reader = None

        # Initialize EasyOCR if available and requested
        if EASYOCR_AVAILABLE and ocr_engine in ['easyocr', 'auto']:
            try:
                # Convert language names to EasyOCR codes
                easyocr_langs = []
                for lang in languages:
                    if lang in EASYOCR_LANGUAGES:
                        easyocr_langs.append(EASYOCR_LANGUAGES[lang])

                if easyocr_langs:
                    self.easyocr_reader = easyocr.Reader(easyocr_langs)
                    logger.info(f"EasyOCR initialized with languages: {easyocr_langs}")
            except Exception as e:
                logger.warning(f"Failed to initialize EasyOCR: {e}")

    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for better OCR accuracy.

        Args:
            image: Input image as numpy array

        Returns:
            Preprocessed image
        """
        # Convert to grayscale if needed
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # Denoising
        denoised = cv2.fastNlMeansDenoising(gray)

        # Adaptive thresholding
        adaptive_thresh = cv2.adaptiveThreshold(
            denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )

        # Morphological cleanup
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        cleaned = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_CLOSE, kernel)

        return cleaned

    def extract_text_tesseract(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using Tesseract OCR.
        """
        if not TESSERACT_AVAILABLE:
            return {'text': '', 'confidence': 0, 'method': 'tesseract_unavailable'}

        try:
            processed_image = self.preprocess_image(image)

            # Build Tesseract language string
            tesseract_langs = []
            for lang in self.languages:
                if lang in OCR_LANGUAGES:
                    tesseract_langs.append(OCR_LANGUAGES[lang])

            lang_string = '+'.join(tesseract_langs) if tesseract_langs else 'eng'

            # Tesseract configuration
            config = r'--oem 3 --psm 6'

            # Extract text
            text = pytesseract.image_to_string(
                processed_image,
                lang=lang_string,
                config=config
            ).strip()

            # Confidence (from per-word data)
            data = pytesseract.image_to_data(
                processed_image, lang=lang_string, output_type=pytesseract.Output.DICT
            )
            confidences = []
            for conf in data.get('conf', []):
                try:
                    c = int(conf)
                    if c > 0:
                        confidences.append(c)
                except Exception:
                    continue
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0

            return {
                'text': text,
                'confidence': avg_confidence,
                'method': 'tesseract',
                'word_count': len(text.split()) if text else 0
            }

        except Exception as e:
            logger.error(f"Tesseract OCR failed: {e}")
            return {'text': '', 'confidence': 0, 'method': 'tesseract_error'}

    def extract_text_easyocr(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using EasyOCR.
        """
        if not self.easyocr_reader:
            return {'text': '', 'confidence': 0, 'method': 'easyocr_unavailable'}

        try:
            # EasyOCR works better with the original image
            results = self.easyocr_reader.readtext(image)

            text_parts = []
            confidences = []

            for (_bbox, text, confidence) in results:
                if confidence > 0.5:  # filter low-confidence detections
                    text_parts.append(text)
                    confidences.append(confidence)

            combined_text = ' '.join(text_parts)
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0

            return {
                'text': combined_text,
                'confidence': avg_confidence * 100,  # Convert to percentage
                'method': 'easyocr',
                'word_count': len(combined_text.split()) if combined_text else 0,
                'detections': len(results)
            }

        except Exception as e:
            logger.error(f"EasyOCR failed: {e}")
            return {'text': '', 'confidence': 0, 'method': 'easyocr_error'}

    def extract_text(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using the best available OCR method.
        """
        results = []

        if self.ocr_engine in ['tesseract', 'auto'] and TESSERACT_AVAILABLE:
            results.append(self.extract_text_tesseract(image))

        if self.ocr_engine in ['easyocr', 'auto'] and self.easyocr_reader:
            results.append(self.extract_text_easyocr(image))

        if not results:
            return {'text': '', 'confidence': 0, 'method': 'no_ocr_available'}

        best_result = max(results, key=lambda x: x['confidence'])
        all_text = ' '.join([r['text'] for r in results if r.get('text')])
        best_result['all_methods_text'] = all_text
        best_result['methods_tried'] = len(results)
        return best_result


def _are_frames_similar(frame1: np.ndarray, frame2: np.ndarray, threshold: float = 0.8) -> bool:
    """
    Check if two frames are similar using histogram correlation (0..1).
    """
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

    hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
    hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])

    corr = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    return corr > threshold


def extract_keyframes(
    video_path: str,
    processed_dir: str,
    video_id: str,
    target_resolution: str = '480p',
    frame_interval: int = 30,          # ~1 per second at 30fps
    similarity_threshold: float = 0.4,  # lower => catch more changes
    ocr_languages: List[str] = ['english'],
    enable_ocr: bool = True,
    max_keyframes: int = 100
) -> Optional[str]:
    """
    Extract keyframes from video with OCR text analysis.
    Returns the directory path containing keyframes and metadata, or None on failure.
    """
    keyframes_dir = None
    cap = None
    try:
        logger.info(f"Extracting keyframes from {video_path}")
        keyframes_dir = os.path.join(processed_dir, f"{video_id}_keyframes")
        os.makedirs(keyframes_dir, exist_ok=True)

        ocr_engine = KeyframeOCR(languages=ocr_languages) if enable_ocr else None
        if enable_ocr:
            logger.info(f"OCR enabled for languages: {ocr_languages}")

        resolution_map = {
            '480p': (854, 480),
            '720p': (1280, 720),
            '1080p': (1920, 1080),
        }
        target_width, target_height = resolution_map.get(target_resolution, (854, 480))

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return None

        fps = cap.get(cv2.CAP_PROP_FPS) or 0.0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        if fps <= 0:
            logger.warning("FPS reported as 0; timestamps will use frame count only.")
        logger.info(f"Video FPS: {fps}, Total frames: {total_frames}")

        keyframes: List[str] = []
        keyframe_metadata: List[Dict[str, Any]] = []
        prev_frame = None
        frame_count = 0
        saved_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % frame_interval == 0:
                if saved_count >= max_keyframes:
                    logger.info(f"Reached maximum keyframe limit ({max_keyframes}), stopping extraction")
                    break

                if prev_frame is None or not _are_frames_similar(frame, prev_frame, similarity_threshold):
                    resized_frame = cv2.resize(frame, (target_width, target_height))
                    filename = f"keyframe_{saved_count:06d}.jpg"
                    filepath = os.path.join(keyframes_dir, filename)
                    cv2.imwrite(filepath, resized_frame)

                    ocr_data: Dict[str, Any] = {}
                    if enable_ocr and ocr_engine:
                        ocr_result = ocr_engine.extract_text(resized_frame)
                        ocr_data = {
                            'text': ocr_result.get('text', ''),
                            'confidence': ocr_result.get('confidence', 0),
                            'method': ocr_result.get('method', 'none'),
                            'word_count': ocr_result.get('word_count', 0)
                        }

                    metadata = {
                        'filename': filename,
                        'filepath': filepath,
                        'frame_number': frame_count,
                        'timestamp': (frame_count / fps) if fps > 0 else None,
                        'resolution': f"{target_width}x{target_height}",
                        'ocr_data': ocr_data
                    }

                    keyframes.append(filepath)
                    keyframe_metadata.append(metadata)
                    prev_frame = resized_frame.copy()
                    saved_count += 1

            frame_count += 1

            if total_frames > 0 and frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                logger.info(
                    f"Progress: {progress:.1f}% - Processed {frame_count}/{total_frames} frames, "
                    f"saved {saved_count} keyframes"
                )
                # Early stop if we hit the configured cap
                if saved_count >= max_keyframes:
                    logger.info("Reached maximum keyframe limit, stopping extraction")
                    break

        # Save metadata
        metadata_file = os.path.join(keyframes_dir, 'keyframes_metadata.json')
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'video_id': video_id,
                    'total_keyframes': len(keyframes),
                    'target_resolution': target_resolution,
                    'frame_interval': frame_interval,
                    'similarity_threshold': similarity_threshold,
                    'ocr_enabled': enable_ocr,
                    'ocr_languages': ocr_languages,
                    'keyframes': keyframe_metadata
                }, f, indent=2, ensure_ascii=False)
            logger.info(f"Keyframe metadata saved to {metadata_file}")
        except Exception as e:
            logger.error(f"Failed to save keyframe metadata: {e}")

        logger.info(f"Extracted {len(keyframes)} keyframes to {keyframes_dir}")

        # Extract and summarize OCR text
        if enable_ocr:
            all_ocr_text: List[str] = []
            high_confidence_text: List[str] = []
            for md in keyframe_metadata:
                ocr_data = md.get('ocr_data', {})
                text = ocr_data.get('text', '')
                confidence = float(ocr_data.get('confidence', 0) or 0)
                if text:
                    all_ocr_text.append(text)
                    if confidence > 70:
                        high_confidence_text.append(text)

            joined_text = ' '.join(all_ocr_text)
            ocr_summary = {
                'total_text_frames': sum(1 for t in all_ocr_text if t),
                'high_confidence_frames': len(high_confidence_text),
                'all_text': joined_text,
                'high_confidence_text': ' '.join(high_confidence_text),
                'word_frequency': dict(Counter(joined_text.split()).most_common(20))
            }

            ocr_summary_file = os.path.join(keyframes_dir, 'ocr_summary.json')
            try:
                with open(ocr_summary_file, 'w', encoding='utf-8') as f:
                    json.dump(ocr_summary, f, indent=2, ensure_ascii=False)
                logger.info(f"OCR summary saved to {ocr_summary_file}")
            except Exception as e:
                logger.error(f"Failed to save OCR summary: {e}")
    except Exception as e:
        logger.error(f"Error extracting keyframes: {e}")
        return None
    finally:
        if cap is not None:
            cap.release()
    return keyframes_dir


def extract_keyframes_for_time_range(video_path: str, start_time: float, end_time: float, max_keyframes: int = 30) -> List[Dict[str, Any]]:
    """
    Extract additional keyframes from a specific time range in the video.
    This ensures we have enough keyframes for smooth video generation.
    """
    cap = None
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"Could not open video: {video_path}")
            return []

        fps = cap.get(cv2.CAP_PROP_FPS) or 0.0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)

        if fps <= 0 or total_frames <= 0:
            logger.error("Invalid video properties (fps/total_frames).")
            return []

        # Calculate frame range for the time segment
        start_frame = int(max(0, start_time) * fps)
        end_frame = int(max(start_time, end_time) * fps)

        # Ensure we don't exceed video bounds
        start_frame = max(0, start_frame)
        end_frame = min(total_frames - 1, end_frame)

        if start_frame >= end_frame:
            return []

        # Calculate frame interval to get desired number of keyframes
        frame_interval = max(1, (end_frame - start_frame) // max(1, max_keyframes))

        keyframes: List[Dict[str, Any]] = []
        current_frame = start_frame

        while current_frame <= end_frame and len(keyframes) < max_keyframes:
            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)
            ret, frame = cap.read()

            if ret:
                # Calculate timestamp for this frame
                timestamp = current_frame / fps

                # Save frame
                frame_filename = f"additional_kf_{len(keyframes):04d}.jpg"
                frame_path = os.path.join(os.path.dirname(video_path), frame_filename)

                # Resize frame for consistency
                frame_resized = cv2.resize(frame, (854, 480))
                cv2.imwrite(frame_path, frame_resized)

                keyframes.append({
                    'filepath': frame_path,
                    'timestamp': timestamp,
                    'confidence': 0.8,  # Default confidence for additional frames
                    'text': '',         # No OCR text for additional frames
                    'type': 'additional'
                })

            current_frame += frame_interval

        logger.info(f"Extracted {len(keyframes)} additional keyframes for time range {start_time}-{end_time}")
        return keyframes

    except Exception as e:
        logger.error(f"Error extracting additional keyframes: {str(e)}")
        return []
    finally:
        if cap is not None:
            cap.release()


def get_keyframe_text_summary(keyframes_dir: str) -> Dict[str, Any]:
    """
    Get OCR text summary for keyframes.

    Args:
        keyframes_dir: Directory containing keyframes

    Returns:
        Dictionary with OCR text summary
    """
    ocr_summary_file = os.path.join(keyframes_dir, 'ocr_summary.json')

    if os.path.exists(ocr_summary_file):
        try:
            with open(ocr_summary_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load OCR summary: {e}")

    return {
        'total_text_frames': 0,
        'high_confidence_frames': 0,
        'all_text': '',
        'high_confidence_text': '',
        'word_frequency': {}
    }
</file>

<file path="app.py">
import os
import json
import time
import logging
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor
import re
from flask import (
    Flask, render_template, request, jsonify, 
    send_from_directory, session, Response, stream_with_context
)
from werkzeug.utils import secure_filename
from werkzeug.exceptions import RequestEntityTooLarge

# Import utility modules
from utils.downloader import is_youtube_url, handle_youtube_download
from utils.transcriber import transcribe_video, SUPPORTED_LANGUAGES as TRANSCRIBE_LANGS
from utils.keyframes import extract_keyframes, get_keyframe_text_summary, extract_keyframes_for_time_range
from utils.clustering import cluster_segments, extract_keywords
from utils.topic_analyzer import analyze_topic_segments, get_keyframes_for_topic, calculate_keyframe_distribution
from utils.summarizer import summarize_cluster
from utils.tts import text_to_speech, SUPPORTED_VOICES, get_available_voices
from utils.video_maker import make_summary_video, SUPPORTED_RESOLUTIONS, select_keyframes_for_topic
from utils.translator import (
    translate_text, translate_segments, get_available_languages,
    detect_language, LANGUAGE_MAPPINGS
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(filename='app.log',encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    'UPLOAD_DIR': 'uploads',
    'PROCESSED_DIR': 'processed',
    'MAX_CONTENT_LENGTH': 40 * 1024 * 1024,  # 40MB
    'MAX_VIDEO_DURATION': 40 * 60,  # 40 minutes in seconds
    'SUPPORTED_EXTENSIONS': {'mp4', 'avi', 'mov', 'mkv', 'webm'},
    'DEFAULT_LANGUAGE': 'en',
    'DEFAULT_VOICE': 'en-US-Standard-C',
    'DEFAULT_RESOLUTION': '480p',
    'ENABLE_DARK_MODE': True,
}

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev-key-change-in-production')
app.config['MAX_CONTENT_LENGTH'] = CONFIG['MAX_CONTENT_LENGTH']
# Ensure directories exist
for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
    os.makedirs(dir_path, exist_ok=True)

# Global state for tracking processing status
processing_status = {}

def get_processing_status(video_id: str) -> Dict:
    """Get the current processing status for a video."""
    return processing_status.get(video_id, {
        'status': 'not_started',
        'progress': 0,
        'message': 'Processing not started',
        'error': None
    })

def update_processing_status(video_id: str, status: str, progress: int, message: str, error: str = None) -> None:
    """Update the processing status for a video."""
    if video_id not in processing_status:
        processing_status[video_id] = {}
    
    processing_status[video_id].update({
        'status': status,
        'progress': progress,
        'message': message,
        'error': error,
        'last_updated': time.time()
    })

@app.route('/test')
def test():
    """Simple test route to verify the app is working."""
    return jsonify({'status': 'ok', 'message': 'Flask app is running'})

@app.route('/')
def index():
    """Render the main page with language and voice options."""
    # Get available voices grouped by language
    voices_by_lang = {}
    available_voices = get_available_voices()
    
    # voices_by_lang will have the structure: {lang_code: {'name': str, 'voices': list}}
    for lang_code, voices in available_voices.items():
        voices_by_lang[lang_code] = {
            'name': TRANSCRIBE_LANGS.get(lang_code, 'Unknown'),
            'voices': voices
        }
    
    # Sort languages by name
    sorted_languages = sorted(
        [(code, data['name']) for code, data in voices_by_lang.items()],
        key=lambda x: x[1]
    )
    
    # Get default language and voice
    default_lang = request.cookies.get('preferred_language', CONFIG['DEFAULT_LANGUAGE'])
    default_voice = request.cookies.get('preferred_voice', CONFIG['DEFAULT_VOICE'])
    dark_mode = request.cookies.get('dark_mode', 'true') == 'true'
    
    return render_template(
        'index.html',
        languages=sorted_languages,
        voices_by_lang=voices_by_lang,
        default_language=default_lang,
        default_voice=default_voice,
        dark_mode=dark_mode,
        max_file_size=CONFIG['MAX_CONTENT_LENGTH'],
        max_duration=CONFIG['MAX_VIDEO_DURATION'],
        supported_extensions=', '.join(CONFIG['SUPPORTED_EXTENSIONS']),
        resolutions=SUPPORTED_RESOLUTIONS,
        default_resolution=CONFIG['DEFAULT_RESOLUTION']
    )

@app.route('/api/process', methods=['POST'])
def process():
    """Handle video processing request with enhanced language support."""
    # Get request data
    source_language = request.form.get('source_language', 'auto')
    target_language = request.form.get('target_language', 'english')
    voice = request.form.get('voice', CONFIG['DEFAULT_VOICE'])
    resolution = request.form.get('resolution', CONFIG['DEFAULT_RESOLUTION'])
    summary_length = request.form.get('summary_length', 'medium')
    enable_ocr = request.form.get('enable_ocr', 'true').lower() == 'true'
    
    # Validate input (either video file or YouTube URL must be provided)
    video_file = request.files.get('video_file')
    yt_url = request.form.get('yt_url', '').strip()
    
    # Debug logging
    logger.info(f"Received video_file: {video_file}")
    logger.info(f"Received yt_url: {yt_url}")
    logger.info(f"All form fields: {list(request.form.keys())}")
    logger.info(f"All files: {list(request.files.keys())}")
    logger.info(f"Request content type: {request.content_type}")
    logger.info(f"Request content length: {request.content_length}")
    
    # Check if video_file exists and has content
    if video_file:
        logger.info(f"Video file details - filename: {video_file.filename}, content_type: {video_file.content_type}")
        if video_file.filename:
            logger.info(f"Video file has filename: {video_file.filename}")
        else:
            logger.info("Video file exists but has no filename")
    else:
        logger.info("No video_file in request.files")
    
    if not video_file and not yt_url:
        logger.error("Validation failed: No video file or YouTube URL provided")
        return jsonify({'error': 'Please provide either a video file or a YouTube URL'}), 400
    
    # Validate languages
    if source_language != 'auto' and source_language not in TRANSCRIBE_LANGS:
        return jsonify({'error': f'Unsupported source language: {source_language}'}), 400
    
    if target_language not in LANGUAGE_MAPPINGS:
        return jsonify({'error': f'Unsupported target language: {target_language}'}), 400
    
    # Generate a unique ID for this processing job
    video_id = str(int(time.time()))
    
    # Initialize processing status
    update_processing_status(video_id, 'initializing', 0, 'Initializing processing...')
    
    # COMPLETELY NEW APPROACH: Save file to disk immediately and pass the path
    video_file_path = None
    yt_url_data = None
    
    if 'video_file' in request.files and request.files['video_file']:
        f = request.files['video_file']
        if f and f.filename:
            try:
                # Generate a temporary file path
                import tempfile
                import uuid
                temp_id = str(uuid.uuid4())
                temp_filename = f"{temp_id}_{f.filename}"
                temp_path = os.path.join(CONFIG['UPLOAD_DIR'], temp_filename)
                
                logger.info(f"Saving uploaded file to temporary path: {temp_path}")
                
                # Save file to disk immediately - no memory operations
                with open(temp_path, 'wb') as temp_file:
                    # Read and write in one operation
                    temp_file.write(f.read())
                
                # Verify file was saved
                if os.path.exists(temp_path) and os.path.getsize(temp_path) > 0:
                    video_file_path = temp_path
                    logger.info(f"File saved successfully: {temp_path} ({os.path.getsize(temp_path)} bytes)")
                else:
                    raise ValueError("Failed to save uploaded file")
                
                # Clear file reference immediately
                f = None
                
            except Exception as e:
                logger.error(f"Failed to save uploaded file: {e}")
                # Clean up any partial file
                if 'temp_path' in locals() and os.path.exists(temp_path):
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                raise ValueError(f"Failed to save uploaded file: {str(e)}")
    
    if 'yt_url' in request.form and request.form['yt_url'].strip():
        yt_url_data = str(request.form['yt_url'].strip())
        logger.info(f"YouTube URL provided: {yt_url_data}")
    
    # Verify we have valid data before starting processing
    if not video_file_path and not yt_url_data:
        raise ValueError("No video file or YouTube URL provided")
    
    logger.info("File handling completed successfully, starting background processing...")
    
    # Force cleanup
    import gc
    gc.collect()
    logger.info("Garbage collection completed")
    
    # Start processing in background
    def process_video():
        nonlocal video_id
        try:
            logger.info(f"Starting background processing for video {video_id}")
            logger.info(f"Video file path available: {video_file_path is not None}")
            logger.info(f"YouTube URL data available: {yt_url_data is not None}")
            
            update_processing_status(video_id, 'downloading', 5, 'Downloading video...')
            
            # 1. Process the uploaded video file or YouTube URL
            try:
                if video_file_path:
                    # We already have the file saved, just use it
                    logger.info(f"Using already saved file: {video_file_path}")
                    video_path = video_file_path
                    # Keep the same video_id for consistency
                elif yt_url_data:
                    # Process YouTube URL
                    logger.info("Processing YouTube URL...")
                    
                    # Create a status callback function that updates the processing status
                    def update_download_status(status, progress, message):
                        update_processing_status(video_id, status, progress, message)
                    
                    video_path, downloaded_video_id = handle_youtube_download(
                        yt_url_data, 
                        CONFIG['UPLOAD_DIR'],
                        video_id=video_id,  # Pass the original video_id
                        status_callback=update_download_status  # Pass the status callback
                    )
                    # Use the downloaded video ID for the file path, but keep the original video_id for status updates
                    logger.info(f"YouTube video downloaded to: {video_path}")
                    logger.info(f"Downloaded video ID: {downloaded_video_id}, Processing video ID: {video_id}")
                    
                    # Status is now updated by the callback, no need to manually update here
                else:
                    raise ValueError("No video file or YouTube URL available")
                
                logger.info(f"Video successfully processed: {video_path}")
                logger.info(f"Moving to transcription stage with video_path: {video_path}")
                
            except Exception as e:
                logger.error(f"Failed to process video: {str(e)}")
                logger.error(f"Exception type: {type(e)}")
                logger.error(f"Exception details: {e}")
                import traceback
                logger.error(f"Full traceback: {traceback.format_exc()}")
                # Update status to error so frontend sees the real error
                update_processing_status(
                    video_id,
                    'error',
                    0,
                    'Failed to download/process video',
                    str(e)
                )
                return  # Stop further processing
            
            # 2. Transcribe the video
            logger.info(f"Starting transcription for video: {video_path}")
            logger.info(f"Video file exists: {os.path.exists(video_path)}")
            logger.info(f"Video file size: {os.path.getsize(video_path) if os.path.exists(video_path) else 'N/A'} bytes")
            
            # Test if ffmpeg is available
            try:
                import subprocess
                result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)
                logger.info(f"FFmpeg available: {result.returncode == 0}")
                if result.returncode != 0:
                    logger.error(f"FFmpeg error: {result.stderr}")
            except Exception as e:
                logger.error(f"FFmpeg test failed: {e}")
            
            update_processing_status(video_id, 'transcribing', 20, 'Transcribing audio...')
            
            # Use auto-detection if source language is auto
            transcription_language = source_language if source_language != 'auto' else 'english'
            logger.info(f"Transcription language: {transcription_language}")
            
            logger.info(f"Calling transcribe_video function...")
            logger.info(f"Function signature: transcribe_video({video_path}, {CONFIG['PROCESSED_DIR']}, {video_id}, {transcription_language})")
            
            # Test if the function is callable
            logger.info(f"transcribe_video function type: {type(transcribe_video)}")
            logger.info(f"transcribe_video function callable: {callable(transcribe_video)}")
            
            try:
                transcript_path, segments = transcribe_video(
                    video_path, 
                    CONFIG['PROCESSED_DIR'], 
                    video_id,
                    language=transcription_language
                )
                logger.info(f"Transcription completed. Transcript path: {transcript_path}, Segments: {len(segments) if segments else 0}")
            except Exception as e:
                logger.error(f"Transcription failed: {str(e)}")
                logger.error(f"Full traceback: {traceback.format_exc()}")
                raise ValueError(f"Transcription failed: {str(e)}")
            
            if not segments:
                raise ValueError("No speech detected in the video")
                
            # Auto-detect language if needed
            detected_language = transcription_language
            if source_language == 'auto' and segments:
                sample_text = ' '.join([seg.get('text', '') for seg in segments[:5]])
                detected_lang = detect_language(sample_text)
                if detected_lang:
                    detected_language = detected_lang
                    logger.info(f"Auto-detected language: {detected_language}")
            
            # Translate segments if source and target languages are different
            if detected_language != target_language:
                update_processing_status(video_id, 'translating', 35, 'Translating content...')
                segments = translate_segments(segments, detected_language, target_language)
                logger.info(f"Translated from {detected_language} to {target_language}")
            
            # 3. Extract key frames with OCR
            update_processing_status(video_id, 'extracting', 40, 'Extracting key frames...')
            
            # Prepare OCR languages
            ocr_languages = [detected_language, target_language]
            if 'english' not in ocr_languages:
                ocr_languages.append('english')
            
            keyframes_dir = extract_keyframes(
                video_path, 
                CONFIG['PROCESSED_DIR'], 
                video_id,
                target_resolution=resolution,
                ocr_languages=ocr_languages,
                enable_ocr=enable_ocr
            )
            
            # Get OCR text summary for additional context
            ocr_summary = {}
            if enable_ocr and keyframes_dir:
                ocr_summary = get_keyframe_text_summary(keyframes_dir)
                if ocr_summary.get('high_confidence_text'):
                    logger.info(f"Extracted OCR text: {len(ocr_summary['high_confidence_text'])} characters")
            
            # 4. Analyze transcript for distinct topics with meaningful names
            update_processing_status(video_id, 'analyzing', 55, 'Analyzing content for distinct topics...')
            
            # Enhance segments with OCR context if available
            if enable_ocr and ocr_summary.get('high_confidence_text'):
                # Add OCR context to the first few segments for better topic analysis
                ocr_context = ocr_summary['high_confidence_text'][:500]  # Limit context
                if segments and ocr_context.strip():
                    segments[0]['text'] += f" [Visual context: {ocr_context}]"
            
            # Use advanced topic analyzer to identify distinct topics with meaningful names
            topics = analyze_topic_segments(
                segments,
                language=target_language
            )
            
            logger.info(f"Identified {len(topics)} distinct topics: {[t['name'] for t in topics]}")
            
            # 5. Process each topic
            summaries = []
            tts_paths = []
            topic_names = []
            topic_keywords = []
            
            for topic in topics:
                topic_id = topic['topic_id']
                topic_name = topic['name']
                topic_segments = topic['segments']
                keywords = topic['keywords']
                
                topic_names.append(topic_name)
                topic_keywords.append(keywords)
                
                # Generate summary in target language
                update_processing_status(
                    video_id, 
                    'summarizing', 
                    60 + (20 * topic_id // len(topics)),
                    f'Generating summary for topic: {topic_name}...'
                )
                
                summary = summarize_cluster(
                    topic_segments,
                    language=target_language,
                    use_extractive=False  # Use abstractive summarization for better quality
                )
                summaries.append(summary)
                
                # Generate TTS audio in target language
                tts_path = text_to_speech(
                    text=summary,
                    output_dir=CONFIG['PROCESSED_DIR'],
                    video_id=video_id,
                    cluster_id=topic_id,
                    voice=voice,
                    language=target_language
                )
                tts_paths.append(tts_path)
            
            # 6. Create summary videos with better organization and intelligent keyframe selection
            summary_videos = []
            
            # Create structured folders for this video
            video_base_dir = os.path.join(CONFIG['PROCESSED_DIR'], video_id)
            os.makedirs(video_base_dir, exist_ok=True)
            
            # Create subdirectories with descriptive names
            keyframes_dir_organized = os.path.join(video_base_dir, 'keyframes')
            audio_dir = os.path.join(video_base_dir, 'audio')
            videos_dir = os.path.join(video_base_dir, 'videos')
            os.makedirs(keyframes_dir_organized, exist_ok=True)
            os.makedirs(audio_dir, exist_ok=True)
            os.makedirs(videos_dir, exist_ok=True)
            
            # Load and organize keyframe metadata
            with open(os.path.join(keyframes_dir, 'keyframes_metadata.json'), 'r', encoding='utf-8') as f:
                keyframe_metadata = json.load(f)['keyframes']
            
            # Copy keyframes to organized directory and update paths
            for kf in keyframe_metadata:
                if os.path.exists(kf['filepath']):
                    new_path = os.path.join(keyframes_dir_organized, os.path.basename(kf['filepath']))
                    import shutil
                    shutil.copy2(kf['filepath'], new_path)
                    kf['filepath'] = new_path
            
            # Save organized metadata with topic information
            organized_metadata_path = os.path.join(video_base_dir, 'metadata.json')
            with open(organized_metadata_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'video_id': video_id,
                    'keyframes': keyframe_metadata,
                    'segments': segments,
                    'topics': [
                        {
                            'topic_id': t['topic_id'],
                            'name': t['name'],
                            'keywords': t['keywords'],
                            'start_time': t['start_time'],
                            'end_time': t['end_time'],
                            'segment_count': len(t['segments'])
                        } for t in topics
                    ]
                }, f, indent=2, ensure_ascii=False)
            
            # Process each topic with intelligent keyframe selection
            failed_topics = []
            for topic_index, topic in enumerate(topics):
                topic_id = topic['topic_id']
                topic_name = topic['name']
                topic_start = topic['start_time']
                topic_end = topic['end_time']
                topic_duration = topic_end - topic_start
                keywords = topic['keywords']

                # Update status with topic name
                update_processing_status(
                    video_id,
                    'rendering',
                    80 + (15 * topic_index // len(topics)),
                    f'Creating video for topic: {topic_name}...'
                )

                # Intelligently select keyframes for this topic based on timestamps
                topic_keyframes = get_keyframes_for_topic(keyframe_metadata, topic_start, topic_end)

                # Calculate optimal number of keyframes based on topic duration
                target_keyframe_count = calculate_keyframe_distribution(
                    topic_duration, 
                    min_keyframes=10, 
                    max_keyframes=40
                )

                logger.info(f"Topic '{topic_name}': Found {len(topic_keyframes)} keyframes, target: {target_keyframe_count}")

                # If we don't have enough keyframes, extract more from the video for this time range
                if len(topic_keyframes) < target_keyframe_count:
                    additional_keyframes = extract_keyframes_for_time_range(
                        video_path, 
                        topic_start, 
                        topic_end, 
                        target_keyframe_count - len(topic_keyframes)
                    )
                    topic_keyframes.extend(additional_keyframes)
                    logger.info(f"Added {len(additional_keyframes)} additional keyframes for topic '{topic_name}'")

                # Move TTS audio to organized directory with descriptive name
                audio_filename = f"{topic_name.replace(' ', '_')}_{topic_id}.wav"
                organized_audio_path = os.path.join(audio_dir, audio_filename)
                import shutil
                try:
                    shutil.copy2(tts_paths[topic_index], organized_audio_path)
                except Exception as e:
                    logger.error(f"Failed to copy TTS audio for topic '{topic_name}': {e}")
                    failed_topics.append(topic_name)
                    summary_videos.append(None)
                    continue

                # Create video with descriptive name
                output_path = os.path.join(videos_dir, f"{topic_name.replace(' ', '_')}_{topic_id}")
                video_out = make_summary_video(
                    keyframes=topic_keyframes,
                    tts_audio_path=organized_audio_path,
                    output_path=output_path,
                    target_width=854,
                    fps=30
                )

                if video_out:
                    # Store the relative path from processed directory
                    video_filename = os.path.relpath(video_out, CONFIG['PROCESSED_DIR'])
                    summary_videos.append(video_filename)

                    # Update metadata with topic-specific info
                    topic_metadata = {
                        'topic_id': topic_id,
                        'start_time': topic_start,
                        'end_time': topic_end,
                        'summary': summaries[topic_index],
                        'keywords': keywords,
                        'keyframes_count': len(topic_keyframes),
                        'audio_file': audio_filename,
                        'video_file': video_filename
                    }
                    # Use topic_id for the metadata file name
                    topic_metadata_path = os.path.join(video_base_dir, f'topic_{topic_id}_metadata.json')
                    with open(topic_metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(topic_metadata, f, indent=2, ensure_ascii=False)
                else:
                    logger.error(f"Failed to create video for topic '{topic_name}'. Keyframes: {len(topic_keyframes)}, Audio: {organized_audio_path}")
                    failed_topics.append(topic_name)
                    summary_videos.append(None)

            logger.info(f"All topic videos processed. Failed topics: {failed_topics if failed_topics else 'None'}")

            # 7. Finalize with enhanced metadata
            result_message = 'Processing complete!'
            if failed_topics:
                result_message += f" WARNING: Failed to generate videos for topics: {', '.join(failed_topics)}."

            result = {
                'video_id': video_id,
                'summaries': summaries,
                'keywords': topic_keywords,
                'summary_videos': summary_videos,
                'source_language': detected_language,
                'target_language': target_language,
                'ocr_enabled': enable_ocr,
                'ocr_summary': ocr_summary if enable_ocr else {},
                'total_topics': len(topics),
                'processing_stats': {
                    'segments_count': len(segments),
                    'keyframes_count': len(os.listdir(keyframes_dir)) if keyframes_dir and os.path.exists(keyframes_dir) else 0,
                    'translation_used': detected_language != target_language
                },
                'status': 'completed',
                'progress': 100,
                'message': result_message
            }

            # Update status with result
            processing_status[video_id].update(result)

            # Mark processing as complete
            update_processing_status(
                video_id,
                'completed',
                100,
                result_message,
                None
            )
            
        except Exception as e:
            logger.error(f"Error processing video: {str(e)}\n{traceback.format_exc()}")
            update_processing_status(
                video_id,
                'error',
                0,
                'An error occurred during processing',
                str(e)
            )
        finally:
            # Cancel all timers if they exist
            if 'timeout_timer' in locals():
                timeout_timer.cancel()
            if 'watchdog_timer' in locals():
                watchdog_timer.cancel()
    
    # Start processing in background with timeout
    from threading import Thread, Timer
    
    def timeout_handler():
        logger.error(f"Processing timeout for video {video_id}")
        update_processing_status(
            video_id,
            'error',
            0,
            'Processing timeout - taking too long',
            'Processing exceeded maximum time limit'
        )
    
    # Set a timeout of 30 minutes for the entire process
    timeout_timer = Timer(30 * 60, timeout_handler)  # 30 minutes
    timeout_timer.start()
    
    # Add a watchdog timer to check if processing is stuck
    def watchdog_check():
        current_status = get_processing_status(video_id)
        if current_status.get('status') == 'downloading' and current_status.get('progress') == 5:
            # If stuck at 5% for more than 5 minutes, force error
            logger.error(f"Processing stuck at 5% for video {video_id}, forcing error")
            update_processing_status(
                video_id,
                'error',
                0,
                'Processing stuck - forcing error',
                'Processing got stuck at download stage'
            )
    
    watchdog_timer = Timer(5 * 60, watchdog_check)  # 5 minutes
    watchdog_timer.start()
    
    thread = Thread(target=process_video)
    thread.daemon = True
    thread.start()
    
    # Return initial response with video ID
    return jsonify({
        'video_id': video_id,
        'status': 'processing',
        'progress': 0,
        'message': 'Processing started'
    })

@app.route('/api/test-upload', methods=['POST'])
def test_upload():
    """Test endpoint to debug file upload issues."""
    logger.info(f"Test upload - files: {list(request.files.keys())}")
    logger.info(f"Test upload - form: {list(request.form.keys())}")
    logger.info(f"Test upload - content type: {request.content_type}")
    logger.info(f"Test upload - content length: {request.content_length}")
    
    if 'video_file' in request.files:
        f = request.files['video_file']
        if f and f.filename:
            # Get file size without seeking to avoid closing the file
            try:
                # Use content_length if available, otherwise read a small chunk to test
                if hasattr(f, 'content_length') and f.content_length:
                    file_size = f.content_length
                else:
                    # Read a small chunk to test if file is readable
                    f.seek(0)
                    test_chunk = f.read(1024)  # Read 1KB
                    file_size = len(test_chunk)
                    if test_chunk:
                        # File is readable, we can estimate size
                        file_size = "Readable (size unknown)"
                    else:
                        file_size = "Empty file"
                
                logger.info(f"Test upload - received file: {f.filename}, size: {file_size}")
                return jsonify({
                    'success': True, 
                    'filename': f.filename,
                    'size': file_size,
                    'content_type': getattr(f, 'content_type', 'unknown')
                })
            except Exception as e:
                logger.error(f"Test upload - error reading file: {e}")
                return jsonify({'success': False, 'error': f'File read error: {str(e)}'})
        else:
            logger.warning("Test upload - video_file exists but is empty or has no filename")
            return jsonify({'success': False, 'error': 'File is empty or has no filename'})
    else:
        logger.warning("Test upload - no video_file in request.files")
        return jsonify({'success': False, 'error': 'No video_file received in request.files'})

@app.route('/api/status/<video_id>')
def get_status(video_id: str):
    """Get the current status of a processing job."""
    status = get_processing_status(video_id)
    logger.info(f"Status request for {video_id}: {status}")
    return jsonify(status)

@app.route('/api/summary/<video_id>')
def get_summary(video_id: str):
    """Get the summary data for a processed video."""
    summary_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_summary.json")
    if os.path.exists(summary_path):
        with open(summary_path, 'r', encoding='utf-8') as f:
            return jsonify(json.load(f))
    return jsonify({'error': 'Summary not found'}), 404

@app.route('/api/stream/<video_id>/<int:cluster_id>')
def stream_video(video_id: str, cluster_id: int):
    """Stream a summary video."""
    video_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_summary_{cluster_id}.mp4")
    
    if not os.path.exists(video_path):
        return jsonify({'error': 'Video not found'}), 404
    
    range_header = request.headers.get('Range', None)
    if not range_header:
        return send_from_directory(
            CONFIG['PROCESSED_DIR'],
            f"{video_id}_summary_{cluster_id}.mp4",
            as_attachment=False,
            mimetype='video/mp4'
        )
    start,end=0,0
    # Handle byte range requests for streaming
    def generate():
        nonlocal start,end
        with open(video_path, 'rb') as f:
            f.seek(0, 2)
            file_size = f.tell()
            start = 0
            end = file_size - 1
            
            # Parse range header
            range_header = request.headers.get('Range')
            if range_header:
                range_match = re.search(r'bytes=(\d*)-(\d*)', range_header)
                if range_match.group(1):
                    start = int(range_match.group(1))
                if range_match.group(2):
                    end = int(range_match.group(2))
            
            chunk_size = 1024 * 1024  # 1MB chunks
            f.seek(start)
            
            while start <= end:
                chunk = f.read(min(chunk_size, end - start + 1))
                if not chunk:
                    break
                yield chunk
                start += len(chunk)
    
    # Send response with appropriate headers
    file_size = os.path.getsize(video_path)
    response = Response(
        stream_with_context(generate()),
        206,  # Partial Content
        mimetype='video/mp4',
        direct_passthrough=True
    )
    
    response.headers.add('Content-Range', f'bytes {start}-{end}/{file_size}')
    response.headers.add('Accept-Ranges', 'bytes')
    response.headers.add('Content-Length', str(end - start + 1))
    
    return response

@app.route('/processed/<path:path>')
def send_processed_file(path):
    """Serve processed files with proper caching headers."""
    response = send_from_directory(CONFIG['PROCESSED_DIR'], path)
    # Cache for 1 day
    response.headers['Cache-Control'] = 'public, max-age=86400'
    return response

@app.route('/srt/<video_id>')
def get_srt(video_id: str):
    """Get SRT subtitles for a video."""
    srt_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}.srt")
    if os.path.exists(srt_path):
        response = send_from_directory(CONFIG['PROCESSED_DIR'], f"{video_id}.srt")
        response.headers['Content-Type'] = 'text/plain; charset=utf-8'
        return response
    return jsonify({'error': 'Subtitles not found'}), 404

@app.route('/keyframes/<video_id>')
def get_keyframes(video_id: str):
    """Get list of keyframes for a video."""
    keyframes_dir = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_keyframes")
    if os.path.exists(keyframes_dir):
        try:
            files = sorted([f for f in os.listdir(keyframes_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
            return jsonify({
                'keyframes': [f"/processed/{video_id}_keyframes/{f}" for f in files]
            })
        except Exception as e:
            logger.error(f"Error listing keyframes: {e}")
            return jsonify({'error': 'Error listing keyframes'}), 500
    return jsonify({'keyframes': []})

def cleanup_old_files():
    """Clean up old temporary files."""
    try:
        now = time.time()
        max_age = 24 * 3600  # 1 day in seconds
        
        for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
            if not os.path.exists(dir_path):
                continue
                
            for filename in os.listdir(dir_path):
                file_path = os.path.join(dir_path, filename)
                try:
                    # Delete files older than max_age
                    if os.path.isfile(file_path):
                        file_age = now - os.path.getmtime(file_path)
                        if file_age > max_age:
                            os.remove(file_path)
                            logger.info(f"Deleted old file: {file_path}")
                    # Clean up old temporary directories
                    elif os.path.isdir(file_path) and file_path.endswith('_temp'):
                        import shutil
                        shutil.rmtree(file_path, ignore_errors=True)
                        logger.info(f"Deleted temp directory: {file_path}")
                except Exception as e:
                    logger.error(f"Error cleaning up {file_path}: {e}")
    except Exception as e:
        logger.error(f"Error in cleanup_old_files: {e}")

if __name__ == '__main__':
    # Ensure directories exist
    for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
        os.makedirs(dir_path, exist_ok=True)
    
    # Set up cleanup job
    from apscheduler.schedulers.background import BackgroundScheduler
    scheduler = BackgroundScheduler()
    scheduler.add_job(func=cleanup_old_files, trigger='interval', hours=1)
    scheduler.start()
    
    # Start the Flask app
    try:
        app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5001)), debug=False, use_reloader=False)
    except (KeyboardInterrupt, SystemExit):
        scheduler.shutdown()
</file>

</files>
